{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68c4bc81",
   "metadata": {},
   "source": [
    "# Homework Assignment 1: Write a Starting Prompt\n",
    "\n",
    "This notebook shows you how to run the first homework example using Galileo.\n",
    "\n",
    "## Configuration\n",
    "\n",
    "To be able to run this notebook, you need to have a Galileo account set up, along with an LLM integration so that your prompt can be run against a model of your choice.\n",
    "\n",
    "1. If you don't have a Galileo account, head to [app.galileo.ai/sign-up](https://app.galileo.ai/sign-up) and sign up for a free account\n",
    "1. Once you have signed up, you will need to configure an LLM integration. Head to the [integrations page](https://app.galileo.ai/settings/integrations) and configure your integration of choice. The notebook assumes you are using OpenAI, but has details on what to change if you are using a different LLM.\n",
    "1. Create a Galileo API key from the [API keys page](https://app.galileo.ai/settings/api-keys)\n",
    "1. In this folder is an example `.env` file called `.env.example`. Copy this file to `.env`, and set the value of `GALILEO_API_KEY` to the API key you just created.\n",
    "1. If you are using a custom Galileo deployment inside your organization, then set the `GALILEO_CONSOLE_URL` environment variable to your console URL. If you are using [app.galileo.ai](https://app.galileo.ai), such as with the free tier, then you can leave this commented out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f1d365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the galileo and python-dotenv package into the current Jupyter kernel\n",
    "%pip install galileo python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aac6f3f",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "To use Galileo, we need to load the API key from the .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60b555f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Check that the GALILEO_API_KEY environment variable is set\n",
    "if not os.getenv(\"GALILEO_API_KEY\"):\n",
    "    raise ValueError(\"GALILEO_API_KEY environment variable is not set. Please set it in your .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040935bd",
   "metadata": {},
   "source": [
    "Next we need to ensure there is a Galileo project set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1df900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from galileo.projects import create_project, get_project\n",
    "\n",
    "PROJECT_NAME = \"AI Evals Course - Homework 1\"\n",
    "project = get_project(name=PROJECT_NAME)\n",
    "if project is None:\n",
    "    project = create_project(name=PROJECT_NAME)\n",
    "\n",
    "print(f\"Using project: {project.name} (ID: {project.id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c7ac17",
   "metadata": {},
   "source": [
    "In this notebook, you will be using the LLM integration you set up in Galileo to run experiments, and generate synthetic data. The default model used is GPT-5.1, and this assumes you have configured an OpenAI integration.\n",
    "\n",
    "If you have another integration set up, or want to use a different model, update this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b883974",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL=\"gpt-5.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f13847d",
   "metadata": {},
   "source": [
    "Finally lets create some unique names for the prompt and dataset, so these can be easily re-run multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d734a8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "PROMPT_NAME = f\"Homework 1 Prompt - {current_time}\"\n",
    "DATASET_NAME = f\"Homework 1 Dataset - {current_time}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a361682c",
   "metadata": {},
   "source": [
    "## Part1: Write an Effective System Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881ccf15",
   "metadata": {},
   "source": [
    "### Create the prompt, and save it to Galileo\n",
    "\n",
    "You can create [prompts](https://v2docs.galileo.ai/sdk-api/experiments/prompts) in Galileo to run against a dataset of inputs. These prompts use [mustache templates](https://mustache.github.io/), and when run against a dataset, the prompt is run against each row, replacing the templates parts of the prompt with values from the dataset.\n",
    "\n",
    "Prompts can be created either as global to an organization, so they can be used by any member, or tied to a specific project. In this case, we will create a global prompt so it can be used in different projects for each homework assignment.\n",
    "\n",
    "Once the prompt is created, a link will be output. You can follow the link to view the prompt in the Galileo console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edec64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from galileo import Message, MessageRole\n",
    "from galileo.prompts import create_prompt, delete_prompt, get_prompt\n",
    "\n",
    "# Define a system prompt. It is this prompt you need to configure\n",
    "system_prompt = \"\"\"\n",
    "You are an expert chef recommending delicious and useful recipes. Present only one recipe at a time. If the user doesn't specify what ingredients they have available, assume only basic ingredients are available.Be descriptive in the steps of the recipe, so it is easy to follow.Have variety in your recipes, don't just recommend the same thing over and over.You MUST suggest a complete recipe; don't ask follow-up questions.Mention the serving size in the recipe. If not specified, assume 2 people.\n",
    "\"\"\"\n",
    "\n",
    "prompt = None\n",
    "\n",
    "# Define a function to create the system prompt in Galileo, then call this.\n",
    "# By using a function, we can easily re-run this code to update the prompt.\n",
    "def set_up_prompt():\n",
    "    \"\"\"\n",
    "    Create a prompt in Galileo using the system prompt defined above\n",
    "    \"\"\"\n",
    "    global prompt\n",
    "\n",
    "    # Start by getting the prompt if it already exists.\n",
    "    # If it does, we can delete it and re-create, if not we create it.\n",
    "    prompt = get_prompt(name=PROMPT_NAME)\n",
    "\n",
    "    if prompt is not None:\n",
    "        print(f\"Prompt already exists with ID: {prompt.id}, deleting it to re-create.\")\n",
    "        prompt = delete_prompt(name=PROMPT_NAME)\n",
    "\n",
    "    prompt = create_prompt(\n",
    "        name=PROMPT_NAME,\n",
    "        template=[\n",
    "            Message(\n",
    "                role=MessageRole.system,\n",
    "                content=system_prompt,\n",
    "            ),\n",
    "            Message(role=MessageRole.user, content=\"{{input}}\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Output a link to view the prompt in Galileo\n",
    "    print(f\"Prompt created. You can view it at {os.environ.get('GALILEO_CONSOLE_URL', 'https://app.galileo.ai/').removesuffix('/')}/prompts/{prompt.id}\")\n",
    "\n",
    "set_up_prompt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde5d408",
   "metadata": {},
   "source": [
    "### Load the dataset and test out the prompt\n",
    "\n",
    "There is a default dataset provided in the original homework. We can use this to test out the prompt.\n",
    "\n",
    "Prompts are run against [datasets](https://v2docs.galileo.ai/sdk-api/experiments/datasets) created and maintained in Galileo. This code creates a Galileo dataset.\n",
    "\n",
    "Like with prompts, datasets can be created at an organization or project level. In this case we will create one global to the organization.\n",
    "\n",
    "Once the dataset is created, a link will be output. You can follow the link to view the dataset in the Galileo console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e72702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# Get the CSV file from the original GitHub repository for the course\n",
    "source_path = \"https://raw.githubusercontent.com/ai-evals-course/recipe-chatbot/refs/heads/main/data/sample_queries.csv\"\n",
    "\n",
    "# Load this csv file into a list of JSON objects with an `input` key\n",
    "with urlopen(source_path) as resp:\n",
    "    lines = (ln.decode(\"utf-8\") for ln in resp)\n",
    "    reader = csv.DictReader(lines)\n",
    "    queries = []\n",
    "    for row in reader:\n",
    "        value = row.get(\"query\")\n",
    "        if not value:\n",
    "            raise ValueError(\"CSV file must have a 'query' column.\")\n",
    "        queries.append({\"input\": value})\n",
    "\n",
    "print(f\"Loaded {len(queries)} queries from {source_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55f3fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from galileo.datasets import get_dataset, create_dataset, delete_dataset\n",
    "\n",
    "# Now we have the CSV file loaded, lets create a dataset. If the dataset already exists, we will delete it and re-create it.\n",
    "dataset = get_dataset(\n",
    "    name=DATASET_NAME\n",
    ")\n",
    "\n",
    "if dataset is not None:\n",
    "    print(f\"Dataset already exists with ID: {dataset.id}, deleting it to re-create.\")\n",
    "    dataset = delete_dataset(\n",
    "        name=DATASET_NAME\n",
    "    )\n",
    "\n",
    "dataset = create_dataset(\n",
    "    name=DATASET_NAME,\n",
    "    content=queries,\n",
    ")\n",
    "\n",
    "print(f\"Dataset created. You can view it at {os.environ.get('GALILEO_CONSOLE_URL', 'https://app.galileo.ai/').removesuffix('/')}/datasets/{dataset.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aec76bb",
   "metadata": {},
   "source": [
    "### Run the prompt against the dataset using an experiment\n",
    "\n",
    "Next we can run the prompt using the dataset in an [experiment](https://v2docs.galileo.ai/sdk-api/experiments/experiments). This will use the LLM integration you have set up earlier, and run each dataset row against the LLM using the prompt provided, saving the output to the experiment.\n",
    "\n",
    "Experiments need a unique name, but if a name is in use then the current date and time is added. This means you can re-run this code and always get a new experiment.\n",
    "\n",
    "This experiment run uses the model defined in the `PromptRunSettings`, using the model name you set earlier.\n",
    "\n",
    "Experiments take time to run. The call to `run_experiment` will return as soon as the experiment has started. The link that is output will show the experiment, and you can monitor its progress from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d953f766",
   "metadata": {},
   "outputs": [],
   "source": [
    "from galileo.experiments import run_experiment\n",
    "from galileo.resources.models import PromptRunSettings\n",
    "\n",
    "# Define a function to run the prompt experiment\n",
    "# By using a function, we can easily re-run this code to start the experiment.\n",
    "def run_experiment_using_prompt_and_dataset():\n",
    "    # Create the experiment prompt run settings to define the model\n",
    "    # Update the model_alias to the model you want to use for the experiment\n",
    "    prompt_run_settings = PromptRunSettings(\n",
    "        model_alias=MODEL\n",
    "    )\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    experiment_name = f\"Homework 1 Experiment - {current_time}\"\n",
    "\n",
    "    # Run the experiment using the prompt and dataset we created\n",
    "    results = run_experiment(\n",
    "        experiment_name,\n",
    "        dataset=dataset,\n",
    "        prompt_template=prompt,\n",
    "        project=PROJECT_NAME,\n",
    "        prompt_settings=prompt_run_settings,\n",
    "    )\n",
    "\n",
    "    print(f\"Experiment has started. You can view the experiment at {results['link']}\")\n",
    "\n",
    "run_experiment_using_prompt_and_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af51cd90",
   "metadata": {},
   "source": [
    "### View the results\n",
    "\n",
    "Once the experiment has completed, you can view the generated output. Each row in the dataset is logged as a separate trace in the experiment.\n",
    "\n",
    "<div>\n",
    "<img src=\"./images/experiment-3-traces.webp\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "Select each row to see the generated response from the prompt and dataset row.\n",
    "\n",
    "<div>\n",
    "<img src=\"./images/experiment-first-trace.webp\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "In addition to the output from the LLM, you will also be able to see the number of tokens used, the time taken, and an estimated cost based off the published token prices for the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b516387",
   "metadata": {},
   "source": [
    "### Improve the system prompt\n",
    "\n",
    "Now that you have a reliable way to run a dataset against your prompt, you can iterate on this to get the system prompt you want. Follow the instructions and examples in the [homework assignment](https://github.com/ai-evals-course/recipe-chatbot/tree/main/homeworks/hw1#part1-write-an-effective-system-prompt) to improve your prompt, and update the `system_prompt` below to reflect your desired changes.\n",
    "\n",
    "Once updated, run the code to regenerate the prompt and run the experiment. Again, a link will be output so you can see the results once the experiment has completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3db5020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the new system prompt\n",
    "system_prompt = \"\"\"\n",
    "You are an expert chef recommending delicious and useful recipes. Present only one recipe at a time. If the user doesn't specify what ingredients they have available, assume only basic ingredients are available.Be descriptive in the steps of the recipe, so it is easy to follow. Have variety in your recipes, don't just recommend the same thing over and over.You MUST suggest a complete recipe; don't ask follow-up questions.\n",
    "\n",
    "Create this recipe in the style of the swedish chef from the Muppets.\n",
    "\"\"\"\n",
    "\n",
    "# Update the prompt in Galileo\n",
    "set_up_prompt()\n",
    "\n",
    "# Run the prompt experiment again with the updated prompt\n",
    "run_experiment_using_prompt_and_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bfe965",
   "metadata": {},
   "source": [
    "## Part 2: Expand and Diversify the Query Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1e3ae6",
   "metadata": {},
   "source": [
    "### Expand the dataset manually\n",
    "\n",
    "You can manually add rows to an existing dataset by passing them in as an array. Update the code below to include more rows, then run it to add these rows to the dataset.\n",
    "\n",
    "Once added, a link to the dataset will be output so you can see the new rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41b738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new rows to the dataset\n",
    "dataset = dataset.add_rows([\n",
    "    {\n",
    "        \"input\": \"I love ramen. Can you suggest a recipe for it using pork belly?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What are some good gluten and dairy free birthday cake ideas?\",\n",
    "    },\n",
    "])\n",
    "\n",
    "print(f\"Dataset extended. You can view it at {os.environ.get('GALILEO_CONSOLE_URL', 'https://app.galileo.ai/').removesuffix('/')}/datasets/{dataset.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a5c6b7",
   "metadata": {},
   "source": [
    "### Expand the dataset with synthetic data\n",
    "\n",
    "Galileo can also generate synthetic data to augment a dataset. This uses whatever LLM integration you have configured, along with a prompt and some examples to generate synthetic data for you. In an ideal world, you would always use production data that is based on real world actions from users, but in the absence of this, you can use synthetic data.\n",
    "\n",
    "Synthetic data generation uses the model passed in to the `extend_dataset` call, which you set earlier.\n",
    "\n",
    "Make sure to review the generated data once complete, to ensure you are happy with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c529fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from galileo.datasets import extend_dataset\n",
    "\n",
    "# Get the original dataset as a simple array to use as examples\n",
    "existing_rows = [v.values[0] for v in dataset.get_content().rows]\n",
    "\n",
    "# Generate synthetic data\n",
    "new_rows = extend_dataset(\n",
    "    prompt_settings={'model_alias': MODEL},\n",
    "    prompt=\"Recipe chatbot\",\n",
    "    instructions=\"\"\"\n",
    "    Write queries to test various aspects of a recipe chatbot. Consider including requests related to:\n",
    "    - Specific cuisines (e.g., Italian pasta dish, Spicy Thai curry)\n",
    "    - Dietary restrictions (e.g., Vegan dessert recipe, Gluten-free breakfast ideas)\n",
    "    - Available ingredients (e.g., What can I make with chicken, rice, and broccoli?)\n",
    "    - Meal types (e.g., Quick lunch for work, Easy dinner for two, Healthy snack for kids)\n",
    "    - Cooking time constraints (e.g., Recipe under 30 minutes)\n",
    "    - Skill levels (e.g., Beginner-friendly baking recipe)\n",
    "    - Vague or ambiguous queries to see how the bot handles them.\n",
    "    \"\"\",\n",
    "    examples=existing_rows,\n",
    "    data_types=['General Query'],\n",
    "    count=10,\n",
    ")\n",
    "\n",
    "# Add the new synthetic rows to the dataset\n",
    "dataset = dataset.add_rows([{\"input\": row.values[0]} for row in new_rows])\n",
    "\n",
    "print(f\"Dataset extended with synthetic data. You can view it at {os.environ.get('GALILEO_CONSOLE_URL', 'https://app.galileo.ai/').removesuffix('/')}/datasets/{dataset.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3980b84",
   "metadata": {},
   "source": [
    "### Save the results as a CSV file\n",
    "\n",
    "The original source data came from a CSV file. We've now extended this to include more rows, so we need to write this back to a CSV file should you want to use it with the original recipe bot app.\n",
    "\n",
    "The original source CSV file was loaded from GitHub. Here we'll just write to the local file system and you can copy the generated file over the CSV file wherever you have cloned the recipe chatbot code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929ed44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_rows = [v.values[0] for v in dataset.get_content().rows]\n",
    "\n",
    "with open(\"sample_queries.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"id\", \"query\"])\n",
    "    for i, val in enumerate(existing_rows, start=1):\n",
    "        writer.writerow([i, val])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eaf44e",
   "metadata": {},
   "source": [
    "## Part 3: Run the Bulk Test & Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb4e5b0",
   "metadata": {},
   "source": [
    "### Run the bulk test script using an experiment\n",
    "\n",
    "The final step asks you to run the bulk test script. This script takes the dataset, uses the system prompt you defined earlier, then runs each row from the sample queries against that prompt, outputting the results.\n",
    "\n",
    "We can do this in a more consistent fashion by running our experiment again with the new dataset. That way the results persist, and we can [compare the experiment runs](https://v2docs.galileo.ai/concepts/experiments/compare) to see the impact of system prompt changes.\n",
    "\n",
    "Once the experiment is started, you will see a link in the output to monitor the progress and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39372597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the prompt experiment again with the updated prompt\n",
    "run_experiment_using_prompt_and_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321ce7a0",
   "metadata": {},
   "source": [
    "## Additional: Run an experiment against the original recipe bot\n",
    "\n",
    "This code sample has a simulation of the recipe bot, using the system prompt and dataset to generate results. This works in this instance as the recipe bot is a simple call to an LLM using the prompt and query from the dataset.\n",
    "\n",
    "In a real world scenario, you would probably want to run your actual application in the experiment, allowing you to evaluate everything that your application has, such as agents, tool calls, and more. Once you have this configured, this becomes something you can run as a unit test, or as part of a CI/CD pipeline. You can then also add evals to this, adding metrics to your experiment to evaluate each response.\n",
    "\n",
    "To do this, you can pass a function to the `run_experiment` call. The steps to do this are:\n",
    "\n",
    "- Define your dataset in advance, either in code or manually in the Galileo console. This becomes more important if you are using this in a CI/CD pipeline, where you can centrally manage a dataset of test cases\n",
    "- Add a unit test to the recipe chatbot. This unit test uses the `run_experiment` call, but instead of passing in a prompt, it passes in a function. This function is the call to the recipe agent, calling the [`get_agent_response` function](https://github.com/ai-evals-course/recipe-chatbot/blob/35618065f209dbf27075b4a4183a986a0c10bd14/backend/utils.py#L31). You would probably need a wrapper function to ensure the dataset row and system prompt are passed in to this function correctly.\n",
    "- Add Galileo logging where applicable inside the app to gather as much detail for the experiment trace as possible.\n",
    "\n",
    "You can read more about running functions as experiments in the [Galileo experiments documentation](https://v2docs.galileo.ai/sdk-api/experiments/running-experiments#run-experiments-against-complex-code-with-custom-functions)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
