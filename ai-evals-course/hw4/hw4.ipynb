{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dad45c08",
   "metadata": {},
   "source": [
    "# Homework Assignment 4: Recipe Bot Retrieval Evaluation\n",
    "\n",
    "This notebook shows you how to run the fourth homework example using Galileo. This homework involves building and evaluating a BM25 retrieval system for recipes.\n",
    "\n",
    "## Configuration\n",
    "\n",
    "To be able to run this notebook, you need to have a Galileo account set up, along with an LLM integration to run an experiment to generate responses.\n",
    "\n",
    "1. If you don't have a Galileo account, head to [app.galileo.ai/sign-up](https://app.galileo.ai/sign-up) and sign up for a free account\n",
    "1. Once you have signed up, you will need to configure an LLM integration. Head to the [integrations page](https://app.galileo.ai/settings/integrations) and configure your integration of choice. The notebook assumes you are using OpenAI, but has details on what to change if you are using a different LLM.\n",
    "1. Create a Galileo API key from the [API keys page](https://app.galileo.ai/settings/api-keys)\n",
    "1. In this folder is an example `.env` file called `.env.example`. Copy this file to `.env`, and set the value of `GALILEO_API_KEY` to the API key you just created.\n",
    "1. If you are using a custom Galileo deployment inside your organization, then set the `GALILEO_CONSOLE_URL` environment variable to your console URL. If you are using [app.galileo.ai](https://app.galileo.ai), such as with the free tier, then you can leave this commented out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f0872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the galileo and python-dotenv package into the current Jupyter kernel\n",
    "%pip install \"galileo\" python-dotenv litellm rank-bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16d1c4f",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "To use Galileo, we need to load the API key from the .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3f0e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Check that the GALILEO_API_KEY environment variable is set\n",
    "if not os.getenv(\"GALILEO_API_KEY\"):\n",
    "    raise ValueError(\"GALILEO_API_KEY environment variable is not set. Please set it in your .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c75fc9",
   "metadata": {},
   "source": [
    "Next we need to ensure there is a Galileo project set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2f03d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from galileo.projects import create_project, get_project\n",
    "\n",
    "PROJECT_NAME = \"AI Evals Course - Homework 4\"\n",
    "project = get_project(name=PROJECT_NAME)\n",
    "if project is None:\n",
    "    project = create_project(name=PROJECT_NAME)\n",
    "\n",
    "print(f\"Using project: {project.name} (ID: {project.id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c6e2f5",
   "metadata": {},
   "source": [
    "## Step 1: Look at your data\n",
    "\n",
    "We'll start by loading the datasets from GitHub and exploring a few rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f735ff47",
   "metadata": {},
   "source": [
    "First we load the recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed6ca51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# Load the source data from the GitHub repository\n",
    "processed_recipes_source_path = \"https://raw.githubusercontent.com/ai-evals-course/recipe-chatbot/refs/heads/main/homeworks/hw4/reference_files/processed_recipes.json\"\n",
    "\n",
    "with urlopen(processed_recipes_source_path) as resp:\n",
    "    recipes = json.load(resp)\n",
    "\n",
    "print(f\"Loaded {len(recipes)} recipes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b01f1e",
   "metadata": {},
   "source": [
    "We can print the first one to see the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76673fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at one recipe\n",
    "recipe = recipes[0]\n",
    "print(f\"Name: {recipe['name']}\")\n",
    "print(f\"Cooking time: {recipe['minutes']} minutes\")\n",
    "print(f\"Ingredients: {recipe['ingredients'][:5]}...\")  # First 5\n",
    "print(f\"Steps: {len(recipe['steps'])} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52b6549",
   "metadata": {},
   "source": [
    "Next we load the synthetic queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e31b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_queries_source_path = \"https://raw.githubusercontent.com/ai-evals-course/recipe-chatbot/refs/heads/main/homeworks/hw4/reference_files/synthetic_queries.jsonl\"\n",
    "\n",
    "with urlopen(synthetic_queries_source_path) as resp:\n",
    "    lines = (ln.decode(\"utf-8\") for ln in resp)\n",
    "    queries = [json.loads(line) for line in lines]\n",
    "\n",
    "print(f\"Loaded {len(queries)} synthetic queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927a20f4",
   "metadata": {},
   "source": [
    "Again we can print the first one to see the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b25f2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at one query\n",
    "q = queries[0]\n",
    "print(f\"Query: {q['query']}\")\n",
    "print(f\"\\nSource recipe: {q['source_recipe_name']}\")\n",
    "print(f\"Source recipe ID: {q['source_recipe_id']}\")\n",
    "print(f\"\\nSalient fact (what makes this query answerable):\\n{q['salient_fact'][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06de3a3",
   "metadata": {},
   "source": [
    "## Step 2: Build BM25 Retriever\n",
    "\n",
    "Now we can build the BM25 retriever in the same way as the original homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc0ec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recipe_to_text(recipe: dict) -> str:\n",
    "    \"\"\"Combine recipe fields into searchable text.\"\"\"\n",
    "    parts = [\n",
    "        recipe['name'],\n",
    "        ' '.join(recipe.get('ingredients', [])),\n",
    "        ' '.join(recipe.get('steps', [])),\n",
    "        ' '.join(recipe.get('tags', []))\n",
    "    ]\n",
    "    return ' '.join(parts).lower()\n",
    "\n",
    "# Create corpus\n",
    "corpus_texts = [recipe_to_text(r) for r in recipes]\n",
    "print(f\"Example text (first 300 chars):\\n{corpus_texts[0][:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8946bd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Simple tokenization (split on whitespace)\n",
    "tokenized_corpus = [text.split() for text in corpus_texts]\n",
    "\n",
    "# Build BM25 index\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "print(f\"BM25 index built with {len(tokenized_corpus)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482c311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query: str, top_k: int = 5) -> list[tuple[dict, float]]:\n",
    "    \"\"\"Retrieve top-k recipes for a query.\n",
    "    \n",
    "    Returns: List of (recipe_index, score, recipe_name)\n",
    "    \"\"\"\n",
    "    tokenized_query = query.lower().split()\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    # Get top-k indices\n",
    "    top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append((recipes[idx], scores[idx]))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6599bf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test it\n",
    "test_query = \"air fryer chicken crispy\"\n",
    "results = retrieve(test_query, top_k=5)\n",
    "\n",
    "print(f\"Query: {test_query}\\n\")\n",
    "print(\"Top 5 results:\")\n",
    "for i, (recipe, score) in enumerate(results, 1):\n",
    "    print(f\"  {i}. {recipe['name']} (score: {score:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4adb42",
   "metadata": {},
   "source": [
    "## Step 2 additional - build a simple chatbot app that uses the retriever and logs to Galileo\n",
    "\n",
    "Evals make more sense in terms of measuring part of an AI interaction. We should just score the `retrieve` function in code, but that doesn't simulate measuring a real world interaction. Instead, let's build a chat function with an LLM that retrieves data from the `retrieve` function to pass to the LLM, simulating a RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c547521d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from galileo import galileo_context\n",
    "from galileo.config import GalileoPythonConfig\n",
    "\n",
    "def process_query(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Process the users query by retrieving relevant recipes and generating a response using the LLM.\n",
    "    \"\"\"\n",
    "    print(f\"Processing query: {query}\")\n",
    "\n",
    "    logger = galileo_context.get_logger_instance()\n",
    "\n",
    "    # Capture the current time in nanoseconds for logging\n",
    "    start_time_ns = datetime.now().timestamp() * 1_000_000_000\n",
    "\n",
    "    # Get the results from the RAG system\n",
    "    results = retrieve(query, top_k=5)\n",
    "\n",
    "    # Log the retrieved recipes to the trace, along all the details of the query\n",
    "    # To act as ground truth for evaluation later\n",
    "    logger.add_retriever_span(\n",
    "        name=\"BM25 Retriever\",\n",
    "        input=query,\n",
    "        output=[recipe for recipe, _ in results],\n",
    "        duration_ns=(datetime.now().timestamp() * 1_000_000_000) - start_time_ns,\n",
    "    )\n",
    "\n",
    "    # Fake an LLM response listing the results\n",
    "    recipes = \"\\n\".join([recipe[\"name\"] for recipe, _ in results])\n",
    "\n",
    "    response = f\"\"\"\n",
    "    Recipes:\n",
    "    {recipes}\n",
    "    \"\"\"\n",
    "\n",
    "    logger.add_llm_span(\n",
    "        name=\"LLM Response\",\n",
    "        input=query,\n",
    "        output=response,\n",
    "        model=\"gpt-5-mini\",\n",
    "        duration_ns=200_000_000,\n",
    "        num_input_tokens=50,\n",
    "        num_output_tokens=150,\n",
    "        total_tokens=200,\n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n",
    "def recipe_chatbot(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Simulate a recipe chatbot interaction.\n",
    "    \"\"\"\n",
    "    print(f\"User -> {query}\")\n",
    "    \n",
    "    # Initialize the Galileo context to log this process\n",
    "    galileo_context.init(project=PROJECT_NAME, log_stream=\"hw4-recipe-chatbot\")\n",
    "\n",
    "    # Start a trace\n",
    "    logger = galileo_context.get_logger_instance()\n",
    "    logger.start_trace(input=query)\n",
    "\n",
    "    response = process_query(query)\n",
    "\n",
    "    # End and flush the trace\n",
    "    logger.conclude(output=response)\n",
    "    logger.flush()\n",
    "\n",
    "    print(f\"Bot -> {response}\")\n",
    "\n",
    "    return response\n",
    "    \n",
    "# Test the process function\n",
    "test_query = queries[0]\n",
    "recipe_chatbot(test_query['query'])\n",
    "\n",
    "# Show Galileo information after the response\n",
    "config = GalileoPythonConfig.get()\n",
    "logger = galileo_context.get_logger_instance()\n",
    "project_url = f\"{config.console_url}project/{logger.project_id}\"\n",
    "log_stream_url = f\"{project_url}/log-streams/{logger.log_stream_id}\"\n",
    "\n",
    "print()\n",
    "print(\"ðŸš€ GALILEO LOG INFORMATION:\")\n",
    "print(f\"ðŸ”— Project   : {project_url}\")\n",
    "print(f\"ðŸ“ Log stream: {log_stream_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9414ff30",
   "metadata": {},
   "source": [
    "Follow the link to the Log stream, and open the first trace. This will show the query, the BM25 retriever, and the response from the LLM.\n",
    "\n",
    "![The trace with a retriever span and LLM span](./images/retriever-trace.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b259ff",
   "metadata": {},
   "source": [
    "## Step 3: Evaluate retrieval\n",
    "\n",
    "We can now evaluate the retrieval using a metric. In this case, we can use a code-based metric, instead of an LLM-as-a-judge metric, as we have access to a ground truth to look up how successful the retrieval was.\n",
    "\n",
    "We can do this using a [local metric](https://v2docs.galileo.ai/concepts/metrics/custom-metrics/custom-metrics-ui-code#local-metrics). This is a metric we can use in an experiment that runs locally. This makes our metric simpler. You can take the local metric and upload it to Galileo later if needed.\n",
    "\n",
    "For this example, we'll just measure the mean reciprocal rank, but you can use the same idea to measure the different Recall@k values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe420ad6",
   "metadata": {},
   "source": [
    "First we need to add the queries to a dataset in Galileo so that our experiments are reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79677a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from galileo.datasets import create_dataset, get_dataset\n",
    "\n",
    "# First check if we already have the dataset. If not, create it.\n",
    "DATASET_NAME = \"hw4-recipe-chatbot-queries\"\n",
    "dataset = get_dataset(name=DATASET_NAME)\n",
    "\n",
    "if dataset is None:\n",
    "    dataset = create_dataset(\n",
    "        name=DATASET_NAME,\n",
    "        content=[\n",
    "            {\n",
    "                \"input\": q[\"query\"],\n",
    "                \"output\": json.dumps(q),\n",
    "            } for q in queries\n",
    "        ],\n",
    "    )\n",
    "\n",
    "print(f\"Dataset: {os.environ.get('GALILEO_CONSOLE_URL', 'https://app.galileo.ai/').removesuffix('/')}/datasets/{dataset.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36ddb5a",
   "metadata": {},
   "source": [
    "The dataset values are passed through the experiment to the retriever span, so we can access the raw ground truth data from the dataset information. This contains the `source_recipe_id` that we can retrieve and use to find the rank of that recipe in the retrieved output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0725c7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from galileo import Trace, Span, StepType\n",
    "from galileo.schema.metrics import LocalMetricConfig\n",
    "\n",
    "def score_retriever_span(step: Span | Trace) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the Mean Reciprocal Rank (MRR) for the retriever step.\n",
    "    \"\"\"\n",
    "    # Look up the query in the dataset\n",
    "    dataset_output = json.loads(step.dataset_output)\n",
    "    ground_truth_recipe_id = dataset_output['source_recipe_id']\n",
    "    \n",
    "    retrieved_ids = [json.loads(r.content)[\"id\"] for r in step.output]\n",
    "    \n",
    "    if ground_truth_recipe_id in retrieved_ids:\n",
    "        rank = retrieved_ids.index(ground_truth_recipe_id) + 1\n",
    "        reciprocal_rank = 1.0 / rank\n",
    "    else:\n",
    "        reciprocal_rank = 0.0\n",
    "\n",
    "    return reciprocal_rank\n",
    "\n",
    "def score_retriever_aggregator(ranks: list[float]) -> float:\n",
    "    \"\"\"\n",
    "    Average the scores. This is used to get an average score across a trace.\n",
    "    \"\"\"\n",
    "    return sum(ranks) / len(ranks)\n",
    "\n",
    "# Create a local metric config that we can pass to the experiment\n",
    "mean_reciprocal_rank = LocalMetricConfig[float](\n",
    "    name=\"Mean Reciprocal Rank\",\n",
    "    scorer_fn=score_retriever_span,\n",
    "    aggregator_fn=score_retriever_aggregator,\n",
    "    scorable_types=[StepType.retriever],\n",
    "    aggregatable_types=[StepType.trace]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355c0216",
   "metadata": {},
   "source": [
    "Now we can run an experiment against the dataset, using our simple 'chatbot' function that incorporates the retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce3352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from galileo.experiments import run_experiment\n",
    "\n",
    "experiment = run_experiment(\n",
    "    experiment_name=\"hw4-recipe-chatbot-experiment\",\n",
    "    project=PROJECT_NAME,\n",
    "    dataset=DATASET_NAME,\n",
    "    function=process_query,\n",
    "    metrics=[mean_reciprocal_rank],\n",
    ")\n",
    "\n",
    "print(f\"ðŸ”— Experiment : {experiment['link']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b018158e",
   "metadata": {},
   "source": [
    "This code prints out a link to the experiment. You can follow the link to see the scores.\n",
    "\n",
    "![Rows in the experiment with calculated MRR](./images/calculated-mrr.webp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
