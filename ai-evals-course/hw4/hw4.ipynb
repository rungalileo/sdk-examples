{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dad45c08",
   "metadata": {},
   "source": [
    "# Homework Assignment 4: Recipe Bot Retrieval Evaluation\n",
    "\n",
    "This notebook shows you how to run the fourth homework example using Galileo. This homework involves building and evaluating a BM25 retrieval system for recipes.\n",
    "\n",
    "## Configuration\n",
    "\n",
    "To be able to run this notebook, you need to have a Galileo account set up, along with an LLM integration to run an experiment to generate responses.\n",
    "\n",
    "1. If you don't have a Galileo account, head to [app.galileo.ai/sign-up](https://app.galileo.ai/sign-up) and sign up for a free account\n",
    "1. Once you have signed up, you will need to configure an LLM integration. Head to the [integrations page](https://app.galileo.ai/settings/integrations) and configure your integration of choice. The notebook assumes you are using OpenAI, but has details on what to change if you are using a different LLM.\n",
    "1. Create a Galileo API key from the [API keys page](https://app.galileo.ai/settings/api-keys)\n",
    "1. In this folder is an example `.env` file called `.env.example`. Copy this file to `.env`, and set the value of `GALILEO_API_KEY` to the API key you just created.\n",
    "1. If you are using a custom Galileo deployment inside your organization, then set the `GALILEO_CONSOLE_URL` environment variable to your console URL. If you are using [app.galileo.ai](https://app.galileo.ai), such as with the free tier, then you can leave this commented out.\n",
    "1. This code uses OpenAI to generate some values. Update the `OPENAI_API_KEY` value in the `.env` file with your OpenAI API key. If you are using another LLM, you will need to update the code to reflect this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69f0872f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (1.2.1)\n",
      "Collecting litellm\n",
      "  Downloading litellm-1.81.5-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting rank-bm25\n",
      "  Using cached rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: galileo[openai] in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (1.44.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from galileo[openai]) (25.4.0)\n",
      "Requirement already satisfied: backoff<3.0.0,>=2.2.1 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from galileo[openai]) (2.2.1)\n",
      "Requirement already satisfied: galileo-core<4.0.0,>=3.82.1 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from galileo[openai]) (3.84.0)\n",
      "Requirement already satisfied: openai in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from galileo[openai]) (2.16.0)\n",
      "Requirement already satisfied: openai-agents in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from galileo[openai]) (0.7.0)\n",
      "Requirement already satisfied: packaging<25.0,>=24.2 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from galileo[openai]) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.12.0 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from galileo[openai]) (2.12.5)\n",
      "Requirement already satisfied: pyjwt<3.0.0,>=2.8.0 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from galileo[openai]) (2.10.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from galileo[openai]) (2.9.0.post0)\n",
      "Requirement already satisfied: wrapt<2.0,>=1.14 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from galileo[openai]) (1.17.3)\n",
      "Requirement already satisfied: httpx<0.29.0,>=0.27.0 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from galileo-core<4.0.0,>=3.82.1->galileo[openai]) (0.28.1)\n",
      "Requirement already satisfied: pydantic-partial<0.11.0,>=0.10.1 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from galileo-core<4.0.0,>=3.82.1->galileo[openai]) (0.10.1)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.2.1 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from galileo-core<4.0.0,>=3.82.1->galileo[openai]) (2.12.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.12.2 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from galileo-core<4.0.0,>=3.82.1->galileo[openai]) (4.15.0)\n",
      "Requirement already satisfied: uvloop<0.22.0,>=0.21.0 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from galileo-core<4.0.0,>=3.82.1->galileo[openai]) (0.21.0)\n",
      "Requirement already satisfied: anyio in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from httpx<0.29.0,>=0.27.0->galileo-core<4.0.0,>=3.82.1->galileo[openai]) (4.12.1)\n",
      "Requirement already satisfied: certifi in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from httpx<0.29.0,>=0.27.0->galileo-core<4.0.0,>=3.82.1->galileo[openai]) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from httpx<0.29.0,>=0.27.0->galileo-core<4.0.0,>=3.82.1->galileo[openai]) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from httpx<0.29.0,>=0.27.0->galileo-core<4.0.0,>=3.82.1->galileo[openai]) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<0.29.0,>=0.27.0->galileo-core<4.0.0,>=3.82.1->galileo[openai]) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.12.0->galileo[openai]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.12.0->galileo[openai]) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.12.0->galileo[openai]) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from python-dateutil<3.0.0,>=2.8.0->galileo[openai]) (1.17.0)\n",
      "Collecting aiohttp>=3.10 (from litellm)\n",
      "  Downloading aiohttp-3.13.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: click in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from litellm) (8.3.1)\n",
      "Collecting fastuuid>=0.13.0 (from litellm)\n",
      "  Downloading fastuuid-0.14.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (1.1 kB)\n",
      "Collecting importlib-metadata>=6.8.0 (from litellm)\n",
      "  Using cached importlib_metadata-8.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting jinja2<4.0.0,>=3.1.2 (from litellm)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.23.0 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from litellm) (4.26.0)\n",
      "Collecting tiktoken>=0.7.0 (from litellm)\n",
      "  Downloading tiktoken-0.12.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting tokenizers (from litellm)\n",
      "  Using cached tokenizers-0.22.2-cp39-abi3-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2<4.0.0,>=3.1.2->litellm)\n",
      "  Using cached markupsafe-3.0.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from jsonschema<5.0.0,>=4.23.0->litellm) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from jsonschema<5.0.0,>=4.23.0->litellm) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from jsonschema<5.0.0,>=4.23.0->litellm) (0.30.0)\n",
      "Collecting numpy (from rank-bm25)\n",
      "  Downloading numpy-2.4.1-cp313-cp313-macosx_14_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp>=3.10->litellm)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp>=3.10->litellm)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp>=3.10->litellm)\n",
      "  Using cached frozenlist-1.8.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp>=3.10->litellm)\n",
      "  Downloading multidict-6.7.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp>=3.10->litellm)\n",
      "  Using cached propcache-0.4.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp>=3.10->litellm)\n",
      "  Using cached yarl-1.22.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (75 kB)\n",
      "Collecting zipp>=3.20 (from importlib-metadata>=6.8.0->litellm)\n",
      "  Using cached zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from openai->galileo[openai]) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from openai->galileo[openai]) (0.12.0)\n",
      "Requirement already satisfied: sniffio in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from openai->galileo[openai]) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from openai->galileo[openai]) (4.67.1)\n",
      "Collecting regex>=2022.1.18 (from tiktoken>=0.7.0->litellm)\n",
      "  Downloading regex-2026.1.15-cp313-cp313-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from tiktoken>=0.7.0->litellm) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (2.6.3)\n",
      "Requirement already satisfied: griffe<2,>=1.5.6 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from openai-agents->galileo[openai]) (1.15.0)\n",
      "Requirement already satisfied: mcp<2,>=1.11.0 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from openai-agents->galileo[openai]) (1.26.0)\n",
      "Requirement already satisfied: types-requests<3,>=2.0 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from openai-agents->galileo[openai]) (2.32.4.20260107)\n",
      "Requirement already satisfied: colorama>=0.4 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from griffe<2,>=1.5.6->openai-agents->galileo[openai]) (0.4.6)\n",
      "Requirement already satisfied: httpx-sse>=0.4 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from mcp<2,>=1.11.0->openai-agents->galileo[openai]) (0.4.3)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from mcp<2,>=1.11.0->openai-agents->galileo[openai]) (0.0.22)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from mcp<2,>=1.11.0->openai-agents->galileo[openai]) (3.2.0)\n",
      "Requirement already satisfied: starlette>=0.27 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from mcp<2,>=1.11.0->openai-agents->galileo[openai]) (0.52.1)\n",
      "Requirement already satisfied: uvicorn>=0.31.1 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from mcp<2,>=1.11.0->openai-agents->galileo[openai]) (0.40.0)\n",
      "Requirement already satisfied: cryptography>=3.4.0 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from pyjwt[crypto]>=2.10.1->mcp<2,>=1.11.0->openai-agents->galileo[openai]) (46.0.4)\n",
      "Requirement already satisfied: cffi>=2.0.0 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->mcp<2,>=1.11.0->openai-agents->galileo[openai]) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from cffi>=2.0.0->cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->mcp<2,>=1.11.0->openai-agents->galileo[openai]) (3.0)\n",
      "Collecting huggingface-hub<2.0,>=0.16.4 (from tokenizers->litellm)\n",
      "  Downloading huggingface_hub-1.3.5-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting filelock (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm)\n",
      "  Using cached filelock-3.20.3-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm)\n",
      "  Using cached fsspec-2026.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm)\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Collecting pyyaml>=5.1 (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm)\n",
      "  Using cached pyyaml-6.0.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
      "Collecting shellingham (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting typer-slim (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm)\n",
      "  Using cached typer_slim-0.21.1-py3-none-any.whl.metadata (16 kB)\n",
      "Downloading litellm-1.81.5-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Downloading aiohttp-3.13.3-cp313-cp313-macosx_11_0_arm64.whl (490 kB)\n",
      "Downloading multidict-6.7.1-cp313-cp313-macosx_11_0_arm64.whl (43 kB)\n",
      "Using cached yarl-1.22.0-cp313-cp313-macosx_11_0_arm64.whl (93 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading fastuuid-0.14.0-cp313-cp313-macosx_11_0_arm64.whl (251 kB)\n",
      "Using cached frozenlist-1.8.0-cp313-cp313-macosx_11_0_arm64.whl (49 kB)\n",
      "Using cached importlib_metadata-8.7.1-py3-none-any.whl (27 kB)\n",
      "Using cached markupsafe-3.0.3-cp313-cp313-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached propcache-0.4.1-cp313-cp313-macosx_11_0_arm64.whl (46 kB)\n",
      "Downloading tiktoken-0.12.0-cp313-cp313-macosx_11_0_arm64.whl (993 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m994.0/994.0 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2026.1.15-cp313-cp313-macosx_11_0_arm64.whl (288 kB)\n",
      "Using cached zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Downloading numpy-2.4.1-cp313-cp313-macosx_14_0_arm64.whl (5.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tokenizers-0.22.2-cp39-abi3-macosx_11_0_arm64.whl (3.0 MB)\n",
      "Downloading huggingface_hub-1.3.5-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Using cached fsspec-2026.1.0-py3-none-any.whl (201 kB)\n",
      "Using cached pyyaml-6.0.3-cp313-cp313-macosx_11_0_arm64.whl (173 kB)\n",
      "Using cached filelock-3.20.3-py3-none-any.whl (16 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached typer_slim-0.21.1-py3-none-any.whl (47 kB)\n",
      "Installing collected packages: zipp, typer-slim, shellingham, regex, pyyaml, propcache, numpy, multidict, MarkupSafe, hf-xet, fsspec, frozenlist, filelock, fastuuid, aiohappyeyeballs, yarl, tiktoken, rank-bm25, jinja2, importlib-metadata, aiosignal, huggingface-hub, aiohttp, tokenizers, litellm\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/25\u001b[0m [litellm]4/25\u001b[0m [litellm]ace-hub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.3 aiohappyeyeballs-2.6.1 aiohttp-3.13.3 aiosignal-1.4.0 fastuuid-0.14.0 filelock-3.20.3 frozenlist-1.8.0 fsspec-2026.1.0 hf-xet-1.2.0 huggingface-hub-1.3.5 importlib-metadata-8.7.1 jinja2-3.1.6 litellm-1.81.5 multidict-6.7.1 numpy-2.4.1 propcache-0.4.1 pyyaml-6.0.3 rank-bm25-0.2.2 regex-2026.1.15 shellingham-1.5.4 tiktoken-0.12.0 tokenizers-0.22.2 typer-slim-0.21.1 yarl-1.22.0 zipp-3.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install the galileo and python-dotenv package into the current Jupyter kernel\n",
    "%pip install \"galileo[openai]\" python-dotenv litellm rank-bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab32048",
   "metadata": {},
   "source": [
    "## Access to the Recipe Chatbot code\n",
    "\n",
    "This homework uses classes from the [Recipe Chatbot](https://github.com/ai-evals-course/recipe-chatbot) codebase. Clone this code and add it to the module path so we can import modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e283c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Clone the repo\n",
    "if not os.path.exists(\"./recipe-chatbot\"):\n",
    "    !git clone https://github.com/ai-evals-course/recipe-chatbot.git\n",
    "\n",
    "# Set the sys path to include the cloned repo\n",
    "import sys\n",
    "sys.path.append(\"./recipe-chatbot\")\n",
    "\n",
    "# Test we can import the relevant modules\n",
    "from backend.query_rewrite_agent import QueryRewriteAgent\n",
    "from backend.retrieval import create_retriever, retrieve_bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16d1c4f",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "To use Galileo, we need to load the API key from the .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db3f0e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Check that the GALILEO_API_KEY environment variable is set\n",
    "if not os.getenv(\"GALILEO_API_KEY\"):\n",
    "    raise ValueError(\"GALILEO_API_KEY environment variable is not set. Please set it in your .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c75fc9",
   "metadata": {},
   "source": [
    "Next we need to ensure there is a Galileo project set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff2f03d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using project: AI Evals Course - Homework 4 (ID: a4a314c7-5a9e-4d6b-9d4c-9e9adceba739)\n"
     ]
    }
   ],
   "source": [
    "from galileo.projects import create_project, get_project\n",
    "\n",
    "PROJECT_NAME = \"AI Evals Course - Homework 4\"\n",
    "project = get_project(name=PROJECT_NAME)\n",
    "if project is None:\n",
    "    project = create_project(name=PROJECT_NAME)\n",
    "\n",
    "print(f\"Using project: {project.name} (ID: {project.id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c6e2f5",
   "metadata": {},
   "source": [
    "## Step 1: Look at your data\n",
    "\n",
    "We'll start by loading the datasets from GitHub and exploring a few rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f735ff47",
   "metadata": {},
   "source": [
    "First we load the recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ed6ca51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200 recipes\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# Load the source data from the GitHub repository\n",
    "processed_recipes_source_path = \"https://raw.githubusercontent.com/ai-evals-course/recipe-chatbot/refs/heads/main/homeworks/hw4/reference_files/processed_recipes.json\"\n",
    "\n",
    "with urlopen(processed_recipes_source_path) as resp:\n",
    "    recipes = json.load(resp)\n",
    "\n",
    "print(f\"Loaded {len(recipes)} recipes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b01f1e",
   "metadata": {},
   "source": [
    "We can print the first one to see the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76673fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 5 cheese crab lasagna with roasted garlic and vegetables\n",
      "Cooking time: 245 minutes\n",
      "Ingredients: ['garlic', 'extra virgin olive oil', 'dry white wine', 'fresh asparagus', 'cooking spray']...\n",
      "Steps: 108 steps\n"
     ]
    }
   ],
   "source": [
    "# Look at one recipe\n",
    "recipe = recipes[0]\n",
    "print(f\"Name: {recipe['name']}\")\n",
    "print(f\"Cooking time: {recipe['minutes']} minutes\")\n",
    "print(f\"Ingredients: {recipe['ingredients'][:5]}...\")  # First 5\n",
    "print(f\"Steps: {len(recipe['steps'])} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52b6549",
   "metadata": {},
   "source": [
    "Next we load the synthetic queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1e31b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200 synthetic queries\n"
     ]
    }
   ],
   "source": [
    "synthetic_queries_source_path = \"https://raw.githubusercontent.com/ai-evals-course/recipe-chatbot/refs/heads/main/homeworks/hw4/reference_files/synthetic_queries.jsonl\"\n",
    "\n",
    "with urlopen(synthetic_queries_source_path) as resp:\n",
    "    lines = (ln.decode(\"utf-8\") for ln in resp)\n",
    "    queries = [json.loads(line) for line in lines]\n",
    "\n",
    "print(f\"Loaded {len(queries)} synthetic queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927a20f4",
   "metadata": {},
   "source": [
    "Again we can print the first one to see the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b25f2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What temperature should I set my oven to and how long do I need to bake this sweet, yeast-based bread for it to turn out fluffy and perfectly cooked?\n",
      "\n",
      "Source recipe: amish friendship bread\n",
      "Source recipe ID: 246125\n",
      "\n",
      "Salient fact (what makes this query answerable):\n",
      "1. **Appliance Settings**: The recipe specifies to \"preheat oven to 325°F,\" which is a precise temperature setting necessary for baking the Amish friendship bread.\n",
      "\n",
      "2. **Timing Specifics**: The recipe indicates a baking time of \"1 hour,\" which is crucial for ensuring the bread is cooked properly and...\n"
     ]
    }
   ],
   "source": [
    "# Look at one query\n",
    "q = queries[0]\n",
    "print(f\"Query: {q['query']}\")\n",
    "print(f\"\\nSource recipe: {q['source_recipe_name']}\")\n",
    "print(f\"Source recipe ID: {q['source_recipe_id']}\")\n",
    "print(f\"\\nSalient fact (what makes this query answerable):\\n{q['salient_fact'][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06de3a3",
   "metadata": {},
   "source": [
    "## Step 2: Build BM25 Retriever\n",
    "\n",
    "Now we can build the BM25 retriever in the same way as the original homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfc0ec6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example text (first 300 chars):\n",
      "5 cheese crab lasagna with roasted garlic and vegetables garlic extra virgin olive oil dry white wine fresh asparagus cooking spray garlic salt salt & freshly ground black pepper red bell peppers fresh basil dry lasagna noodles roma tomatoes dried oregano parmesan-romano cheese mix butter sweet onio...\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def recipe_to_text(recipe: Dict) -> str:\n",
    "    \"\"\"Combine recipe fields into searchable text.\"\"\"\n",
    "    parts = [\n",
    "        recipe['name'],\n",
    "        ' '.join(recipe.get('ingredients', [])),\n",
    "        ' '.join(recipe.get('steps', [])),\n",
    "        ' '.join(recipe.get('tags', []))\n",
    "    ]\n",
    "    return ' '.join(parts).lower()\n",
    "\n",
    "# Create corpus\n",
    "corpus_texts = [recipe_to_text(r) for r in recipes]\n",
    "print(f\"Example text (first 300 chars):\\n{corpus_texts[0][:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8946bd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 index built with 200 documents\n"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Simple tokenization (split on whitespace)\n",
    "tokenized_corpus = [text.split() for text in corpus_texts]\n",
    "\n",
    "# Build BM25 index\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "print(f\"BM25 index built with {len(tokenized_corpus)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "482c311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "def retrieve(query: str, top_k: int = 5) -> List[Tuple[int, float, str]]:\n",
    "    \"\"\"Retrieve top-k recipes for a query.\n",
    "    \n",
    "    Returns: List of (recipe_index, score, recipe_name)\n",
    "    \"\"\"\n",
    "    tokenized_query = query.lower().split()\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    # Get top-k indices\n",
    "    top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append((recipes[idx]['id'], scores[idx], recipes[idx]['name']))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6599bf9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: air fryer chicken crispy\n",
      "\n",
      "Top 5 results:\n",
      "  1. 7 layer elote dip (score: 6.75)\n",
      "  2. amazingly juicy grilled lemon chicken (score: 6.19)\n",
      "  3. algerian chicken preserved lemon bourek (score: 5.78)\n",
      "  4. a grape picker s lunch sausages and lentils with thyme and wine (score: 5.57)\n",
      "  5. alton s french onion soup attacked by sandi (score: 5.54)\n"
     ]
    }
   ],
   "source": [
    "# Test it\n",
    "test_query = \"air fryer chicken crispy\"\n",
    "results = retrieve(test_query, top_k=5)\n",
    "\n",
    "print(f\"Query: {test_query}\\n\")\n",
    "print(\"Top 5 results:\")\n",
    "for i, (recipe_id, score, name) in enumerate(results, 1):\n",
    "    print(f\"  {i}. {name} (score: {score:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4adb42",
   "metadata": {},
   "source": [
    "## Step 2 additional - build a simple chatbot app that uses the retriever and logs to Galileo\n",
    "\n",
    "Evals make more sense in terms of measuring part of an AI interaction. We should just score the `retrieve` function in code, but that doesn't simulate measuring a real world interaction. Instead, let's build a chat function with an LLM that retrieves data from the `retrieve` function to pass to the LLM, simulating a RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c547521d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query: str) -> str:\n",
    "    # Get the results from the RAG system\n",
    "    results = retrieve(query, top_k=5)\n",
    "    print(f\"Query: {test_query}\\n\")\n",
    "    print(\"Top 5 results:\")\n",
    "    for i, (recipe_id, score, name) in enumerate(results, 1):\n",
    "        print(f\"  {i}. {name} (score: {score:.2f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
