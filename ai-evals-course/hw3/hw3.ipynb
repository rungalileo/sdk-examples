{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bdee923",
   "metadata": {},
   "source": [
    "# Homework Assignment 3: LLM-as-Judge for Recipe Bot Evaluation\n",
    "\n",
    "This notebook shows you how to run the third homework example using Galileo. This homework involves creating your own LLM-as-a-judge prompt to validate if the response from the recipe chatbot follows the users dietary restrictions.\n",
    "\n",
    "This homework example has three possible starting points, and this notebook is taking option three, starting from an already labelled data set. If you want to start with the other options, work through them to create the labelled data set, then use that instead of the pre-created labelled data set.\n",
    "\n",
    "## Configuration\n",
    "\n",
    "To be able to run this notebook, you need to have a Galileo account set up, along with an LLM integration to run an experiment to generate responses.\n",
    "\n",
    "1. If you don't have a Galileo account, head to [app.galileo.ai/sign-up](https://app.galileo.ai/sign-up) and sign up for a free account\n",
    "1. Once you have signed up, you will need to configure an LLM integration. Head to the [integrations page](https://app.galileo.ai/settings/integrations) and configure your integration of choice. The notebook assumes you are using OpenAI, but has details on what to change if you are using a different LLM.\n",
    "1. Create a Galileo API key from the [API keys page](https://app.galileo.ai/settings/api-keys)\n",
    "1. In this folder is an example `.env` file called `.env.example`. Copy this file to `.env`, and set the value of `GALILEO_API_KEY` to the API key you just created.\n",
    "1. If you are using a custom Galileo deployment inside your organization, then set the `GALILEO_CONSOLE_URL` environment variable to your console URL. If you are using [app.galileo.ai](https://app.galileo.ai), such as with the free tier, then you can leave this commented out.\n",
    "1. This code uses OpenAI to generate some values. Update the `OPENAI_API_KEY` value in the `.env` file with your OpenAI API key. If you are using another LLM, you will need to update the code to reflect this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e3bf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the galileo and python-dotenv package into the current Jupyter kernel\n",
    "%pip install \"galileo[openai]\" python-dotenv pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c88c830",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "To use Galileo, we need to load the API key from the .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de277e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Check that the GALILEO_API_KEY environment variable is set\n",
    "if not os.getenv(\"GALILEO_API_KEY\"):\n",
    "    raise ValueError(\"GALILEO_API_KEY environment variable is not set. Please set it in your .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67044c89",
   "metadata": {},
   "source": [
    "Next we need to ensure there is a Galileo project set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909e5b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from galileo.projects import create_project, get_project\n",
    "\n",
    "PROJECT_NAME = \"AI Evals Course - Homework 3\"\n",
    "project = get_project(name=PROJECT_NAME)\n",
    "if project is None:\n",
    "    project = create_project(name=PROJECT_NAME)\n",
    "\n",
    "print(f\"Using project: {project.name} (ID: {project.id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7243820e",
   "metadata": {},
   "source": [
    "In this notebook, you will be using the LLM integration you set up in Galileo to run an experiment, as well as calling OpenAI directly to generate some data. The default model used is GPT-5.1, and this assumes you have configured an OpenAI integration.\n",
    "\n",
    "If you have another integration set up, or want to use a different model, update this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76da60ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL=\"gpt-5.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005f47e3",
   "metadata": {},
   "source": [
    "## Step 2: Split your data (skipping step 1)\n",
    "\n",
    "We are starting at step 2, using the already labelled data set. We'll start by loading the data set, and divide into pass and fail sets. We divide into pass and fail to run each as a separate experiment, so that it is easier to see the true positive and true negative traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5054679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# Load the labelled traces\n",
    "source_path = \"https://raw.githubusercontent.com/ai-evals-course/recipe-chatbot/refs/heads/main/homeworks/hw3/reference_files/labeled_traces.jsonl\"\n",
    "\n",
    "# Open and read the labelled traces into a JSON array\n",
    "with urlopen(source_path) as resp:\n",
    "    lines = (ln.decode(\"utf-8\") for ln in resp)\n",
    "    labelled_traces = [json.loads(line) for line in lines]\n",
    "\n",
    "# Divide into pass and fail sets. These are defined by the label property as PASS or FAIL.\n",
    "passed_traces = [trace for trace in labelled_traces if trace[\"label\"] == \"PASS\"]\n",
    "failed_traces = [trace for trace in labelled_traces if trace[\"label\"] == \"FAIL\"]\n",
    "\n",
    "print(f\"Total traces: {len(labelled_traces)}\")\n",
    "print(f\"Passed traces: {len(passed_traces)}\")\n",
    "print(f\"Failed traces: {len(failed_traces)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d064c259",
   "metadata": {},
   "source": [
    "Now for each set, split into 10% train, 40% dev, and 50% test. Do this randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fb8e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def split_traces(traces, train_frac=0.10, dev_frac=0.40, seed=42, name=None):\n",
    "    rng = random.Random(seed)\n",
    "    shuffled = traces.copy()\n",
    "    rng.shuffle(shuffled)\n",
    "    total = len(shuffled)\n",
    "    train_size = int(total * train_frac)\n",
    "    dev_size = int(total * dev_frac)\n",
    "    train = shuffled[:train_size]\n",
    "    dev = shuffled[train_size:train_size + dev_size]\n",
    "    test = shuffled[train_size + dev_size:]\n",
    "    label = f\"{name} \" if name else \"\"\n",
    "    print(f\"{label}split: Train={len(train)} Dev={len(dev)} Test={len(test)} (total={total})\")\n",
    "    return train, dev, test\n",
    "\n",
    "# Split passed and failed traces into train/dev/test sets\n",
    "passed_train, passed_dev, passed_test = split_traces(passed_traces, name=\"Passed\")\n",
    "failed_train, failed_dev, failed_test = split_traces(failed_traces, name=\"Failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fbadbf",
   "metadata": {},
   "source": [
    "To make it easier to view the data, let's upload these as datasets in Galileo. First let's create unique names for these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd9646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "PASSED_TRAINING_SET_NAME = f\"Homework 3 Passed training set - {current_time}\"\n",
    "PASSED_DEV_SET_NAME = f\"Homework 3 Passed dev set - {current_time}\"\n",
    "PASSED_TEST_SET_NAME = f\"Homework 3 Passed test set - {current_time}\"\n",
    "FAILED_TRAINING_SET_NAME = f\"Homework 3 Failed training set - {current_time}\"\n",
    "FAILED_DEV_SET_NAME = f\"Homework 3 Failed dev set - {current_time}\"\n",
    "FAILED_TEST_SET_NAME = f\"Homework 3 Failed test set - {current_time}\"\n",
    "\n",
    "print(f\"Passed training set name: {PASSED_TRAINING_SET_NAME}\")\n",
    "print(f\"Passed dev set name: {PASSED_DEV_SET_NAME}\")\n",
    "print(f\"Passed test set name: {PASSED_TEST_SET_NAME}\")\n",
    "print(f\"Failed training set name: {FAILED_TRAINING_SET_NAME}\")\n",
    "print(f\"Failed dev set name: {FAILED_DEV_SET_NAME}\")\n",
    "print(f\"Failed test set name: {FAILED_TEST_SET_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d226189d",
   "metadata": {},
   "source": [
    "Next we create the actual datasets, with the query, response, and some additional information as metadata, such as the reasoning behind the label.\n",
    "\n",
    "A link to the datasets is output after they are created, so you can view these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb89c549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from galileo.datasets import get_dataset, create_dataset, delete_dataset\n",
    "\n",
    "def create_or_replace_dataset(dataset_name, rows):\n",
    "\n",
    "    # Now we have the CSV file loaded, lets create a dataset. If the dataset already exists, we will delete it and re-create it.\n",
    "    dataset = get_dataset(\n",
    "        name=dataset_name\n",
    "    )\n",
    "\n",
    "    if dataset is not None:\n",
    "        print(f\"Dataset already exists with ID: {dataset.id}, deleting it to re-create.\")\n",
    "        dataset = delete_dataset(\n",
    "            name=dataset_name\n",
    "        )\n",
    "\n",
    "    dataset = create_dataset(\n",
    "        name=dataset_name,\n",
    "        content=[\n",
    "            {\n",
    "                \"input\": row[\"query\"],\n",
    "                \"output\": row[\"response\"],\n",
    "                \"metadata\": {\n",
    "                    \"query_id\": row[\"query_id\"],\n",
    "                    \"reasoning\": row[\"reasoning\"],\n",
    "                },\n",
    "            } for row in rows\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Create the datasets\n",
    "passed_training_dataset = create_or_replace_dataset(\n",
    "    dataset_name=PASSED_TRAINING_SET_NAME,\n",
    "    rows=passed_train\n",
    ")\n",
    "passed_dev_dataset = create_or_replace_dataset(\n",
    "    dataset_name=PASSED_DEV_SET_NAME,\n",
    "    rows=passed_dev\n",
    ")\n",
    "passed_test_dataset = create_or_replace_dataset(\n",
    "    dataset_name=PASSED_TEST_SET_NAME,\n",
    "    rows=passed_test\n",
    ")\n",
    "failed_training_dataset = create_or_replace_dataset(\n",
    "    dataset_name=FAILED_TRAINING_SET_NAME,\n",
    "    rows=failed_train\n",
    ")\n",
    "failed_dev_dataset = create_or_replace_dataset(\n",
    "    dataset_name=FAILED_DEV_SET_NAME,\n",
    "    rows=failed_dev\n",
    ")\n",
    "failed_test_dataset = create_or_replace_dataset(\n",
    "    dataset_name=FAILED_TEST_SET_NAME,\n",
    "    rows=failed_test\n",
    ")\n",
    "\n",
    "print(\"Training datasets - refer to these when getting examples for your prompt:\")\n",
    "print(f\"Passed training dataset created. You can view it at {os.environ.get('GALILEO_CONSOLE_URL', 'https://app.galileo.ai/').removesuffix('/')}/datasets/{passed_training_dataset.id}\")\n",
    "print(f\"Failed training dataset created. You can view it at {os.environ.get('GALILEO_CONSOLE_URL', 'https://app.galileo.ai/').removesuffix('/')}/datasets/{failed_training_dataset.id}\")\n",
    "\n",
    "print(\"Dev datasets - these will be used to test your prompt during development:\")\n",
    "print(f\"Passed dev dataset created. You can view it at {os.environ.get('GALILEO_CONSOLE_URL', 'https://app.galileo.ai/').removesuffix('/')}/datasets/{passed_dev_dataset.id}\")\n",
    "print(f\"Failed dev dataset created. You can view it at {os.environ.get('GALILEO_CONSOLE_URL', 'https://app.galileo.ai/').removesuffix('/')}/datasets/{failed_dev_dataset.id}\")\n",
    "\n",
    "print(\"Test datasets - these will be used to evaluate your final prompt:\")\n",
    "print(f\"Passed test dataset created. You can view it at {os.environ.get('GALILEO_CONSOLE_URL', 'https://app.galileo.ai/').removesuffix('/')}/datasets/{passed_test_dataset.id}\")\n",
    "print(f\"Failed test dataset created. You can view it at {os.environ.get('GALILEO_CONSOLE_URL', 'https://app.galileo.ai/').removesuffix('/')}/datasets/{failed_test_dataset.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d229d08a",
   "metadata": {},
   "source": [
    "## Step 3: Write your judge prompt\n",
    "\n",
    "Now we have the train, dev, and test data sets, we can build the LLM-as-a-judge prompt.\n",
    "\n",
    "Update the `custom_metric_prompt` below with your judge prompt. Remember to include:\n",
    "- The task and criterion\n",
    "- Clear Pass/Fail definitions\n",
    "- 2-3 few-shot examples from your Train set with input, output, reasoning, and pass/fail label. Refer to the datasets created in the last section for these.\n",
    "\n",
    "For the expected output, the metric should return `true` if the output follows the dietary restrictions defined in the input, otherwise return `false`. You also do not need to ask for reasoning, this is handled automatically by Galileo.\n",
    "\n",
    "This prompt will be used by Galileo to evaluate the outputs. Refer to the [LLM-as-a-judge prompt engineering guide in the Galileo documentation](https://v2docs.galileo.ai/concepts/metrics/custom-metrics/prompt-engineering) for more guidance on how to structure a good LLM-as-a-judge prompt.\n",
    "\n",
    "> Instead of creating this metric in code, you can also create and test it in the Galileo console, including using Prompt Assist to get the prompt created for you. You can read more in the [Galileo custom LLM-as-a-judge metrics documentation](https://v2docs.galileo.ai/concepts/metrics/custom-metrics/custom-metrics-ui-llm#create-a-new-llm-as-a-judge-metric-in-the-galileo-console)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbdc3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The prompt for the custom dietary adherence metric.\n",
    "# Make sure to fill in the examples section with relevant examples from the training datasets, with both pass and fail examples.\n",
    "custom_metric_prompt = \"\"\"\n",
    "You are an expert nutritionist and dietary specialist evaluating whether recipe responses properly adhere to specified dietary restrictions.\n",
    "\n",
    "DIETARY RESTRICTION DEFINITIONS:\n",
    "- Vegan: No animal products (meat, dairy, eggs, honey, etc.)\n",
    "- Vegetarian: No meat or fish, but dairy and eggs are allowed\n",
    "- Gluten-free: No wheat, barley, rye, or other gluten-containing grains\n",
    "- Dairy-free: No milk, cheese, butter, yogurt, or other dairy products\n",
    "- Keto: Very low carb (typically <20g net carbs), high fat, moderate protein\n",
    "- Paleo: No grains, legumes, dairy, refined sugar, or processed foods\n",
    "- Pescatarian: No meat except fish and seafood\n",
    "- Kosher: Follows Jewish dietary laws (no pork, shellfish, mixing meat/dairy)\n",
    "- Halal: Follows Islamic dietary laws (no pork, alcohol, proper slaughter)\n",
    "- Nut-free: No tree nuts or peanuts\n",
    "- Low-carb: Significantly reduced carbohydrates (typically <50g per day)\n",
    "- Sugar-free: No added sugars or high-sugar ingredients\n",
    "- Raw vegan: Vegan foods not heated above 118°F (48°C)\n",
    "- Whole30: No grains, dairy, legumes, sugar, alcohol, or processed foods\n",
    "- Diabetic-friendly: Low glycemic index, controlled carbohydrates\n",
    "- Low-sodium: Reduced sodium content for heart health\n",
    "\n",
    "Rubric:\n",
    "- true: The recipe in the output clearly adheres to the dietary preferences defined in the input with appropriate ingredients and preparation methods\n",
    "- false: The recipe in the output contains ingredients or methods that violate the dietary preferences defined in the input\n",
    "- Consider both explicit ingredients and cooking methods\n",
    "\n",
    "Here are some examples of how to evaluate dietary adherence:\n",
    "\n",
    "true:\n",
    "\n",
    "User asks for vegan pasta → Bot suggests nutritional yeast instead of parmesan\n",
    "User asks for gluten-free bread → Bot uses almond flour and xanthan gum\n",
    "User asks for keto dinner → Bot provides cauliflower rice with high-fat protein\n",
    "\n",
    "false:\n",
    "\n",
    "User asks for vegan pasta → Bot includes honey (not vegan)\n",
    "User asks for gluten-free bread → Bot uses regular soy sauce (contains wheat)\n",
    "User asks for keto dinner → Bot includes sweet potato (too many carbs)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a631784",
   "metadata": {},
   "source": [
    "Once you are happy with the prompt, it can be used to create a custom metric in Galileo. This metric will be a boolean metric, and will operate at the trace level, so that it assesses the results of the end to end user question to answer flow, for example including all agents and tools that a recipe bot might use in the process. The `cot_enabled` parameter turns on reasoning, so you get an explanation with each result. This also uses the model from the `MODEL` constant you set earlier, and runs this against the LLM 3 times to get a consensus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4a237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from galileo.metrics import create_custom_llm_metric, OutputTypeEnum, StepType, delete_metric\n",
    "from galileo.scorers import Scorers\n",
    "\n",
    "METRIC_NAME = \"Dietary requirements adherence\"\n",
    "\n",
    "if len(Scorers().list(name=METRIC_NAME)) > 0:\n",
    "    print(f\"Metric '{METRIC_NAME}' already exists. Deleting it to re-create.\")\n",
    "    delete_metric(name=METRIC_NAME)\n",
    "\n",
    "# Create the metric\n",
    "metric = create_custom_llm_metric(\n",
    "    name=METRIC_NAME,\n",
    "    user_prompt=custom_metric_prompt,\n",
    "    node_level=StepType.trace,\n",
    "    cot_enabled=True,\n",
    "    model_name=MODEL,\n",
    "    num_judges=3,\n",
    "    description=\"\"\"\n",
    "This metric determines if the response from the recipe bot adheres to the\n",
    "specified dietary requirements of the user.\n",
    "\"\"\",\n",
    "    output_type=OutputTypeEnum.BOOLEAN,\n",
    ")\n",
    "\n",
    "print(f\"Custom metric created. You can view it at {os.environ.get('GALILEO_CONSOLE_URL', 'https://app.galileo.ai/').removesuffix('/')}/metrics/{metric.scorer_id}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffca067",
   "metadata": {},
   "source": [
    "## Step 4: Test and refine\n",
    "\n",
    "The next step is to test the new custom metric on the dev sets, then use the results of this to refine the prompt. This testing will run an experiment against the pass and fail dev datasets, and use these to calculate the True Positive Rate (TPR) and the True Negative Rate (TNR). The TPR is the percentage of rows from the pass dev dataset that actually passes the metric, and the TNR is the percentage of rows from the fail dev dataset that actually fail the metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af088ea5",
   "metadata": {},
   "source": [
    "### Test the metric\n",
    "\n",
    "Experiments can be run against a prompt or a custom function (such as one that calls an agent). In this case the outputs are already defined in our source data, so we need to define a custom function that takes the dataset row, and looks up then returns the output. It will also create the relevant LLM span to simulate the LLM returning the response. We can then use this in the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c43233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from galileo import galileo_context\n",
    "\n",
    "def mock_llm_function(input: str) -> str:\n",
    "    # Get the logger to log an LLM span\n",
    "    logger = galileo_context.get_logger_instance()\n",
    "\n",
    "    # Look up the response for the given input from the labelled traces\n",
    "    row = [trace for trace in labelled_traces if trace[\"query\"] == input][0]\n",
    "    \n",
    "    # Log the LLM span\n",
    "    logger.add_llm_span(input=input, output=row[\"response\"], model=MODEL, name=\"Recipe generation\")\n",
    "\n",
    "    # Return the response\n",
    "    return row[\"response\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7647a56",
   "metadata": {},
   "source": [
    "Next we can define some unique names for the 2 experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2726d19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "TPR_DEV_EXPERIMENT_NAME = f\"Homework 3 TPR Experiment (dev) - {current_time}\"\n",
    "TNR_DEV_EXPERIMENT_NAME = f\"Homework 3 TNR Experiment (dev) - {current_time}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9727169",
   "metadata": {},
   "source": [
    "And define a function to run the experiment, then wait for the results to be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1b8f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from galileo.experiments import get_experiment, run_experiment\n",
    "\n",
    "def run_dietary_adherence_experiment(experiment_name, dataset):\n",
    "    # Run the experiment\n",
    "    experiment_response = run_experiment(\n",
    "        experiment_name=experiment_name,\n",
    "        dataset=dataset,\n",
    "        function=mock_llm_function,\n",
    "        metrics=[METRIC_NAME],\n",
    "        project=PROJECT_NAME,  \n",
    "    )\n",
    "\n",
    "    # Poll until we have the metrics calculated - waiting 5 seconds between polls\n",
    "    experiment = get_experiment(\n",
    "        project_id=experiment_response[\"experiment\"].project_id,\n",
    "        experiment_name=experiment_response[\"experiment\"].name,\n",
    "    )\n",
    "    while (\n",
    "        experiment.aggregate_metrics is None\n",
    "        or f\"average_{METRIC_NAME}\" not in experiment.aggregate_metrics\n",
    "    ):\n",
    "        # If we don't have the metrics calculated, Sleep for 5 seconds before polling again\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Reload the experiment to see if we have the metrics\n",
    "        experiment = get_experiment(\n",
    "            project_id=experiment_response[\"experiment\"].project_id,\n",
    "            experiment_name=experiment_response[\"experiment\"].name,\n",
    "        )\n",
    "    \n",
    "    return experiment, experiment_response['link']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f2566e",
   "metadata": {},
   "source": [
    "We'll start by running the pass dev dataset to get the TPR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96930fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment, link = run_dietary_adherence_experiment(\n",
    "    experiment_name=TPR_DEV_EXPERIMENT_NAME,\n",
    "    dataset=passed_dev_dataset,\n",
    ")\n",
    "\n",
    "true_positive_rate = experiment.aggregate_metrics[f\"average_{METRIC_NAME}\"]\n",
    "\n",
    "print(f\"True Positive Rate (TPR) on passed dev set: {true_positive_rate:.2%}\")\n",
    "print(f\"You can view the experiment at {link}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1967d428",
   "metadata": {},
   "source": [
    "Next we can do the same with the fail dev set to get the TNR. As the metric measures adherence to dietary requirements, we instead have to calculate this as the number that fail, so 1 - the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaac5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment, link = run_dietary_adherence_experiment(\n",
    "    experiment_name=TNR_DEV_EXPERIMENT_NAME,\n",
    "    dataset=failed_dev_dataset,\n",
    ")\n",
    "\n",
    "# This metric measures what passes the dietary requirements check, so the true negative rate is 1 - the metric value\n",
    "true_negative_rate = 1.0 - experiment.aggregate_metrics[f\"average_{METRIC_NAME}\"]\n",
    "\n",
    "print(f\"True Negative Rate (TNR) on failed dev set: {true_negative_rate:.2%}\")\n",
    "print(f\"You can view the experiment at {link}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bb545b",
   "metadata": {},
   "source": [
    "### Refine the prompt\n",
    "\n",
    "Ideally we want the true positive and true negative rates to be as close to 100% as possible. Refine the `custom_metric_prompt`, then re-run the code to recreate the metric.\n",
    "\n",
    "Once recreated, re-run the experiments to get the updated TPR and TNR. Keep refining until you have a prompt you are happy with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccdcae3",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate new traces\n",
    "\n",
    "Now we have a working metric, we can run it on the test set to get a better idea how it is working. We can repeat the same experiments with the test dataset, and hopefully we should now get a good score for the TPR and TNR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d086ae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "TPR_TEST_EXPERIMENT_NAME = f\"Homework 3 TPR Experiment (test) - {current_time}\"\n",
    "TNR_TEST_EXPERIMENT_NAME = f\"Homework 3 TNR Experiment (test) - {current_time}\"\n",
    "\n",
    "tpr_experiment, tpr_link = run_dietary_adherence_experiment(\n",
    "    experiment_name=TPR_TEST_EXPERIMENT_NAME,\n",
    "    dataset=passed_test_dataset,\n",
    ")\n",
    "\n",
    "tnr_experiment, tnr_link = run_dietary_adherence_experiment(\n",
    "    experiment_name=TNR_TEST_EXPERIMENT_NAME,\n",
    "    dataset=failed_test_dataset,\n",
    ")\n",
    "\n",
    "true_positive_rate = tpr_experiment.aggregate_metrics[f\"average_{METRIC_NAME}\"]\n",
    "true_negative_rate = 1.0 - tnr_experiment.aggregate_metrics[f\"average_{METRIC_NAME}\"]\n",
    "\n",
    "print(f\"True Positive Rate (TPR) on passed test set: {true_positive_rate:.2%}\")\n",
    "print(f\"True Negative Rate (TNR) on failed test set: {true_negative_rate:.2%}\")\n",
    "print(f\"You can view the TPR experiment at {tpr_link}\")\n",
    "print(f\"You can view the TNR experiment at {tnr_link}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
