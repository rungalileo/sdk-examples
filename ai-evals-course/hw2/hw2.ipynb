{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be4a89ca",
   "metadata": {},
   "source": [
    "# Homework Assignment 2: Recipe Bot Error Analysis\n",
    "\n",
    "This notebook shows you how to run the second homework example using Galileo.\n",
    "\n",
    "## Configuration\n",
    "\n",
    "To be able to run this notebook, you need to have a Galileo account set up, along with an LLM integration to run an experiment to generate responses.\n",
    "\n",
    "1. If you don't have a Galileo account, head to [app.galileo.ai/sign-up](https://app.galileo.ai/sign-up) and sign up for a free account\n",
    "1. Once you have signed up, you will need to configure an LLM integration. Head to the [integrations page](https://app.galileo.ai/settings/integrations) and configure your integration of choice. The notebook assumes you are using OpenAI, but has details on what to change if you are using a different LLM.\n",
    "1. Create a Galileo API key from the [API keys page](https://app.galileo.ai/settings/api-keys)\n",
    "1. In this folder is an example `.env` file called `.env.example`. Copy this file to `.env`, and set the value of `GALILEO_API_KEY` to the API key you just created.\n",
    "1. If you are using a custom Galileo deployment inside your organization, then set the `GALILEO_CONSOLE_URL` environment variable to your console URL. If you are using [app.galileo.ai](https://app.galileo.ai), such as with the free tier, then you can leave this commented out.\n",
    "1. This code uses OpenAI to generate some values. Update the `OPENAI_API_KEY` value in the `.env` file with your OpenAI API key. If you are using another LLM, you will need to update the code to reflect this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5663c223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (1.2.1)\n",
      "Requirement already satisfied: galileo[openai] in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (1.37.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from galileo[openai]) (25.4.0)\n",
      "Requirement already satisfied: backoff<3.0.0,>=2.2.1 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from galileo[openai]) (2.2.1)\n",
      "Requirement already satisfied: galileo-core<4.0.0,>=3.75.0 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from galileo[openai]) (3.76.0)\n",
      "Collecting openai (from galileo[openai])\n",
      "  Downloading openai-2.13.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting openai-agents (from galileo[openai])\n",
      "  Downloading openai_agents-0.6.3-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting packaging<25.0,>=24.2 (from galileo[openai])\n",
      "  Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.11.7 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from galileo[openai]) (2.12.5)\n",
      "Requirement already satisfied: pyjwt<3.0.0,>=2.8.0 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from galileo[openai]) (2.10.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from galileo[openai]) (2.9.0.post0)\n",
      "Requirement already satisfied: wrapt<2.0,>=1.14 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from galileo[openai]) (1.17.3)\n",
      "Requirement already satisfied: httpx<0.29.0,>=0.27.0 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from galileo-core<4.0.0,>=3.75.0->galileo[openai]) (0.28.1)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.2.1 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from galileo-core<4.0.0,>=3.75.0->galileo[openai]) (2.12.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.12.2 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from galileo-core<4.0.0,>=3.75.0->galileo[openai]) (4.15.0)\n",
      "Requirement already satisfied: uvloop<0.22.0,>=0.21.0 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from galileo-core<4.0.0,>=3.75.0->galileo[openai]) (0.21.0)\n",
      "Requirement already satisfied: anyio in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from httpx<0.29.0,>=0.27.0->galileo-core<4.0.0,>=3.75.0->galileo[openai]) (4.12.0)\n",
      "Requirement already satisfied: certifi in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from httpx<0.29.0,>=0.27.0->galileo-core<4.0.0,>=3.75.0->galileo[openai]) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from httpx<0.29.0,>=0.27.0->galileo-core<4.0.0,>=3.75.0->galileo[openai]) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from httpx<0.29.0,>=0.27.0->galileo-core<4.0.0,>=3.75.0->galileo[openai]) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<0.29.0,>=0.27.0->galileo-core<4.0.0,>=3.75.0->galileo[openai]) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.11.7->galileo[openai]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.11.7->galileo[openai]) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.11.7->galileo[openai]) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jimbobbennett/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages (from python-dateutil<3.0.0,>=2.8.0->galileo[openai]) (1.17.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai->galileo[openai])\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.10.0 (from openai->galileo[openai])\n",
      "  Downloading jiter-0.12.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting sniffio (from openai->galileo[openai])\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai->galileo[openai])\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting griffe<2,>=1.5.6 (from openai-agents->galileo[openai])\n",
      "  Using cached griffe-1.15.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting mcp<2,>=1.11.0 (from openai-agents->galileo[openai])\n",
      "  Downloading mcp-1.24.0-py3-none-any.whl.metadata (89 kB)\n",
      "Collecting requests<3,>=2.0 (from openai-agents->galileo[openai])\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting types-requests<3,>=2.0 (from openai-agents->galileo[openai])\n",
      "  Using cached types_requests-2.32.4.20250913-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting colorama>=0.4 (from griffe<2,>=1.5.6->openai-agents->galileo[openai])\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting httpx-sse>=0.4 (from mcp<2,>=1.11.0->openai-agents->galileo[openai])\n",
      "  Using cached httpx_sse-0.4.3-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting jsonschema>=4.20.0 (from mcp<2,>=1.11.0->openai-agents->galileo[openai])\n",
      "  Using cached jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting python-multipart>=0.0.9 (from mcp<2,>=1.11.0->openai-agents->galileo[openai])\n",
      "  Using cached python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting sse-starlette>=1.6.1 (from mcp<2,>=1.11.0->openai-agents->galileo[openai])\n",
      "  Downloading sse_starlette-3.0.4-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting starlette>=0.27 (from mcp<2,>=1.11.0->openai-agents->galileo[openai])\n",
      "  Using cached starlette-0.50.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting uvicorn>=0.31.1 (from mcp<2,>=1.11.0->openai-agents->galileo[openai])\n",
      "  Using cached uvicorn-0.38.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests<3,>=2.0->openai-agents->galileo[openai])\n",
      "  Using cached charset_normalizer-3.4.4-cp313-cp313-macosx_10_13_universal2.whl.metadata (37 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.0->openai-agents->galileo[openai])\n",
      "  Using cached urllib3-2.6.2-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.20.0->mcp<2,>=1.11.0->openai-agents->galileo[openai])\n",
      "  Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=4.20.0->mcp<2,>=1.11.0->openai-agents->galileo[openai])\n",
      "  Using cached referencing-0.37.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=4.20.0->mcp<2,>=1.11.0->openai-agents->galileo[openai])\n",
      "  Using cached rpds_py-0.30.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting cryptography>=3.4.0 (from pyjwt[crypto]>=2.10.1->mcp<2,>=1.11.0->openai-agents->galileo[openai])\n",
      "  Using cached cryptography-46.0.3-cp311-abi3-macosx_10_9_universal2.whl.metadata (5.7 kB)\n",
      "Collecting cffi>=2.0.0 (from cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->mcp<2,>=1.11.0->openai-agents->galileo[openai])\n",
      "  Using cached cffi-2.0.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.6 kB)\n",
      "Collecting pycparser (from cffi>=2.0.0->cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->mcp<2,>=1.11.0->openai-agents->galileo[openai])\n",
      "  Using cached pycparser-2.23-py3-none-any.whl.metadata (993 bytes)\n",
      "Collecting click>=7.0 (from uvicorn>=0.31.1->mcp<2,>=1.11.0->openai-agents->galileo[openai])\n",
      "  Using cached click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Using cached packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Downloading openai-2.13.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m824.8 kB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.12.0-cp313-cp313-macosx_11_0_arm64.whl (318 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading openai_agents-0.6.3-py3-none-any.whl (239 kB)\n",
      "Using cached griffe-1.15.0-py3-none-any.whl (150 kB)\n",
      "Downloading mcp-1.24.0-py3-none-any.whl (232 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.4-cp313-cp313-macosx_10_13_universal2.whl (208 kB)\n",
      "Using cached types_requests-2.32.4.20250913-py3-none-any.whl (20 kB)\n",
      "Using cached urllib3-2.6.2-py3-none-any.whl (131 kB)\n",
      "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Using cached httpx_sse-0.4.3-py3-none-any.whl (9.0 kB)\n",
      "Using cached jsonschema-4.25.1-py3-none-any.whl (90 kB)\n",
      "Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)\n",
      "Using cached cryptography-46.0.3-cp311-abi3-macosx_10_9_universal2.whl (7.2 MB)\n",
      "Using cached cffi-2.0.0-cp313-cp313-macosx_11_0_arm64.whl (181 kB)\n",
      "Using cached python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Using cached referencing-0.37.0-py3-none-any.whl (26 kB)\n",
      "Using cached rpds_py-0.30.0-cp313-cp313-macosx_11_0_arm64.whl (358 kB)\n",
      "Downloading sse_starlette-3.0.4-py3-none-any.whl (11 kB)\n",
      "Using cached starlette-0.50.0-py3-none-any.whl (74 kB)\n",
      "Using cached uvicorn-0.38.0-py3-none-any.whl (68 kB)\n",
      "Using cached click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Using cached pycparser-2.23-py3-none-any.whl (118 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: urllib3, tqdm, sniffio, rpds-py, python-multipart, pycparser, packaging, jiter, httpx-sse, distro, colorama, click, charset_normalizer, uvicorn, types-requests, starlette, requests, referencing, griffe, cffi, sse-starlette, openai, jsonschema-specifications, cryptography, jsonschema, mcp, openai-agents\n",
      "\u001b[2K  Attempting uninstall: packaging\n",
      "\u001b[2K    Found existing installation: packaging 25.0\n",
      "\u001b[2K    Uninstalling packaging-25.0:\n",
      "\u001b[2K      Successfully uninstalled packaging-25.0\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27/27\u001b[0m [openai-agents]0m [mcp]tography]\n",
      "\u001b[1A\u001b[2KSuccessfully installed cffi-2.0.0 charset_normalizer-3.4.4 click-8.3.1 colorama-0.4.6 cryptography-46.0.3 distro-1.9.0 griffe-1.15.0 httpx-sse-0.4.3 jiter-0.12.0 jsonschema-4.25.1 jsonschema-specifications-2025.9.1 mcp-1.24.0 openai-2.13.0 openai-agents-0.6.3 packaging-24.2 pycparser-2.23 python-multipart-0.0.20 referencing-0.37.0 requests-2.32.5 rpds-py-0.30.0 sniffio-1.3.1 sse-starlette-3.0.4 starlette-0.50.0 tqdm-4.67.1 types-requests-2.32.4.20250913 urllib3-2.6.2 uvicorn-0.38.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install the galileo and python-dotenv package into the current Jupyter kernel\n",
    "%pip install \"galileo[openai]\" python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bc6af9",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "To use Galileo, we need to load the API key from the .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "920b7940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Check that the GALILEO_API_KEY environment variable is set\n",
    "if not os.getenv(\"GALILEO_API_KEY\"):\n",
    "    raise ValueError(\"GALILEO_API_KEY environment variable is not set. Please set it in your .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aacbbb4",
   "metadata": {},
   "source": [
    "Next we need to ensure there is a Galileo project set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00ab1989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using project: AI Evals Course - Homework 1 (ID: b54a29e8-7c14-436f-ba65-34318928d1ca)\n"
     ]
    }
   ],
   "source": [
    "from galileo.projects import create_project, get_project\n",
    "\n",
    "PROJECT_NAME = \"AI Evals Course - Homework 1\"\n",
    "project = get_project(name=PROJECT_NAME)\n",
    "if project is None:\n",
    "    project = create_project(name=PROJECT_NAME)\n",
    "\n",
    "print(f\"Using project: {project.name} (ID: {project.id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfce86f",
   "metadata": {},
   "source": [
    "In this notebook, you will be using the LLM integration you set up in Galileo to run an experiment, as well as calling OpenAI directly to generate some data. The default model used is GPT-5.1, and this assumes you have configured an OpenAI integration.\n",
    "\n",
    "If you have another integration set up, or want to use a different model, update this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7616e5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL=\"gpt-5.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151d8337",
   "metadata": {},
   "source": [
    "## Part 1: Generate Test Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee96250",
   "metadata": {},
   "source": [
    "### Pick your dimensions\n",
    "\n",
    "Pick your dimensions that matter for your test queries, such as cuisine, dietary restrictions, meal type etc. Then add example values, ideally three values for each dimension.\n",
    "\n",
    "Update the code below to reflect these dimensions and example values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "768e94ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimensions for the recipe generation task, along with some example values\n",
    "dimensions = [\n",
    "    {\n",
    "        \"name\": \"cuisine\",\n",
    "        \"values:\": [\"Italian\", \"Chinese\", \"Mexican\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"dietary restrictions\",\n",
    "        \"values:\": [\"Vegetarian\", \"Vegan\", \"Gluten-Free\", \"Diabetic\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"meal type\",\n",
    "        \"values:\": [\"Breakfast\", \"Lunch\", \"Dinner\", \"Snack\"]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d0c0e2",
   "metadata": {},
   "source": [
    "### Create combinations\n",
    "\n",
    "You can use an LLM to generate queries using combinations of the different dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "906c8ec5",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mast\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m client = \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mOPENAI_API_KEY\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Create a prompt to generate test queries using the dimensions\u001b[39;00m\n\u001b[32m      7\u001b[39m prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33mGenerate 20 diverse test queries for a recipe bot. Use combinations of the following dimensions:\u001b[39m\n\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m \u001b[38;5;132;01m{\u001b[39;00mdimensions\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m \u001b[33m[\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mquery 1\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mquery 2\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, ...]\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[33m\"\"\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/sdk-examples/ai-evals-course/.venv/lib/python3.13/site-packages/openai/_client.py:137\u001b[39m, in \u001b[36mOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    135\u001b[39m     api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    138\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    139\u001b[39m     )\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(api_key):\n\u001b[32m    141\u001b[39m     \u001b[38;5;28mself\u001b[39m.api_key = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import ast\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Create a prompt to generate test queries using the dimensions\n",
    "prompt = f\"\"\"Generate 20 diverse test queries for a recipe bot. Use combinations of the following dimensions:\n",
    "\n",
    "{dimensions}\n",
    "\n",
    "The queries should be natural language questions that users might ask a recipe bot, incorporating different combinations of the dimension values provided.\n",
    "\n",
    "Return ONLY a valid Python list of strings, with no additional text or explanation. For example:\n",
    "[\"query 1\", \"query 2\", ...]\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that generates test queries. Return only valid Python lists.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "# Extract the response and convert to Python array\n",
    "test_queries = ast.literal_eval(response.choices[0].message.content)\n",
    "\n",
    "print(f\"Generated {len(test_queries)} test queries:\")\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"{i}. {query}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
