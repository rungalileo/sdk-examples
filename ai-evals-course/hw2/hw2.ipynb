{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be4a89ca",
   "metadata": {},
   "source": [
    "# Homework Assignment 2: Recipe Bot Error Analysis\n",
    "\n",
    "This notebook shows you how to run the second homework example using Galileo.\n",
    "\n",
    "## Configuration\n",
    "\n",
    "To be able to run this notebook, you need to have a Galileo account set up, along with an LLM integration to run an experiment to generate responses.\n",
    "\n",
    "1. If you don't have a Galileo account, head to [app.galileo.ai/sign-up](https://app.galileo.ai/sign-up) and sign up for a free account\n",
    "1. Once you have signed up, you will need to configure an LLM integration. Head to the [integrations page](https://app.galileo.ai/settings/integrations) and configure your integration of choice. The notebook assumes you are using OpenAI, but has details on what to change if you are using a different LLM.\n",
    "1. Create a Galileo API key from the [API keys page](https://app.galileo.ai/settings/api-keys)\n",
    "1. In this folder is an example `.env` file called `.env.example`. Copy this file to `.env`, and set the value of `GALILEO_API_KEY` to the API key you just created.\n",
    "1. If you are using a custom Galileo deployment inside your organization, then set the `GALILEO_CONSOLE_URL` environment variable to your console URL. If you are using [app.galileo.ai](https://app.galileo.ai), such as with the free tier, then you can leave this commented out.\n",
    "1. This code uses OpenAI to generate some values. Update the `OPENAI_API_KEY` value in the `.env` file with your OpenAI API key. If you are using another LLM, you will need to update the code to reflect this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5663c223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the galileo and python-dotenv package into the current Jupyter kernel\n",
    "%pip install \"galileo[openai]\" python-dotenv pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bc6af9",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "To use Galileo, we need to load the API key from the .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920b7940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Check that the GALILEO_API_KEY environment variable is set\n",
    "if not os.getenv(\"GALILEO_API_KEY\"):\n",
    "    raise ValueError(\"GALILEO_API_KEY environment variable is not set. Please set it in your .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aacbbb4",
   "metadata": {},
   "source": [
    "Next we need to ensure there is a Galileo project set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ab1989",
   "metadata": {},
   "outputs": [],
   "source": [
    "from galileo.projects import create_project, get_project\n",
    "\n",
    "PROJECT_NAME = \"AI Evals Course - Homework 2\"\n",
    "project = get_project(name=PROJECT_NAME)\n",
    "if project is None:\n",
    "    project = create_project(name=PROJECT_NAME)\n",
    "\n",
    "print(f\"Using project: {project.name} (ID: {project.id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfce86f",
   "metadata": {},
   "source": [
    "In this notebook, you will be using the LLM integration you set up in Galileo to run an experiment, as well as calling OpenAI directly to generate some data. The default model used is GPT-5.1, and this assumes you have configured an OpenAI integration.\n",
    "\n",
    "If you have another integration set up, or want to use a different model, update this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7616e5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL=\"gpt-5.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151d8337",
   "metadata": {},
   "source": [
    "## Part 1: Generate Test Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee96250",
   "metadata": {},
   "source": [
    "### Pick your dimensions\n",
    "\n",
    "Pick your dimensions that matter for your test queries, such as cuisine, dietary restrictions, meal type etc. Then add example values, ideally three values for each dimension.\n",
    "\n",
    "Update the code below to reflect these dimensions and example values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768e94ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimensions for the recipe generation task, along with some example values\n",
    "# Update this to reflect the dimensions you want to test\n",
    "dimensions = [\n",
    "    {\n",
    "        \"name\": \"cuisine\",\n",
    "        \"values:\": [\"Italian\", \"Chinese\", \"Mexican\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"dietary restrictions\",\n",
    "        \"values:\": [\"Vegetarian\", \"Vegan\", \"Gluten-Free\", \"Diabetic\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"meal type\",\n",
    "        \"values:\": [\"Breakfast\", \"Lunch\", \"Dinner\", \"Snack\"]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d0c0e2",
   "metadata": {},
   "source": [
    "### Create combinations\n",
    "\n",
    "You can use an LLM to generate queries using combinations of the different dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906c8ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "from pprint import pprint\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Create a prompt to generate test queries using the dimensions\n",
    "prompt = f\"\"\"Generate 20 unique combinations of the following dimensions:\n",
    "\n",
    "{dimensions}\n",
    "\n",
    "Ensure these combinations cover a diverse range of scenarios, and are realistic.\n",
    "\n",
    "Return JSON with an array of objects. These objects contain all the dimension names as keys, and the selected value for that dimension as the value.\n",
    "\"\"\"\n",
    "\n",
    "# Get the response from OpenAI\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that generates combinations of values. Return only valid Python JSON arrays.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Extract the response and parse the JSON\n",
    "combinations = json.loads(response.choices[0].message.content)\n",
    "print(\"Generated Combinations:\\n\")\n",
    "pprint(combinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6708e52",
   "metadata": {},
   "source": [
    "### Turn Combinations into Queries\n",
    "\n",
    "Now we have our combinations, we can use these with an LLM to create queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec88f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt to generate test queries using the dimensions\n",
    "prompt = f\"\"\"Generate 7 queries that can be run against a recipe generation model using the following combinations of criteria. Each row in this JSON represents a different combination of criteria, so pick 7 rows randomly from this list, and use them to generate the queries.\n",
    "\n",
    "{combinations}\n",
    "\n",
    "Ensure these queries are realistic queries that a user might ask a recipe generation model. The user may be experienced with interacting with an LLM, or may not be, so vary the complexity of the queries. Also vary based on a selection of ages, writing styles, tones, or skills with English.\n",
    "\n",
    "Return JSON with an array of queries. This JSON should be a simple array of strings, with each string being a different query.\n",
    "\"\"\"\n",
    "\n",
    "# Get the response from OpenAI\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that generates test queries. Return only valid Python lists.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Get the list of queries\n",
    "queries = [str(q) for q in json.loads(response.choices[0].message.content)]\n",
    "print(\"Generated Queries:\\n\")\n",
    "pprint(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3734a2",
   "metadata": {},
   "source": [
    "## Part 2: Find and Categorize Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542fccbd",
   "metadata": {},
   "source": [
    "### Run Your Bot\n",
    "\n",
    "Just link in the [previous homework](../hw1/README.md), we will be using an experiment in Galileo to run the recipe bot and generate the output. This bot is a simple LLM call, so we can replicate this using an experiment with a prompt and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e75cf47",
   "metadata": {},
   "source": [
    "Let's start by creating some unique names for the prompt and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ca1702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "PROMPT_NAME = f\"Homework 2 Prompt - {current_time}\"\n",
    "DATASET_NAME = f\"Homework 2 Dataset - {current_time}\"\n",
    "\n",
    "print(f\"Prompt name: {PROMPT_NAME}\")\n",
    "print(f\"Dataset name: {DATASET_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a4e258",
   "metadata": {},
   "source": [
    "Now let's generate the prompt in Galileo. This uses the basic prompt from the recipe chatbot starting example, so make sure to update this to reflect the prompt you generated in [homework 1](../hw1/hw1.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3c4ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from galileo import Message, MessageRole\n",
    "from galileo.prompts import create_prompt, delete_prompt, get_prompt\n",
    "\n",
    "# Define a system prompt. It is this prompt you need to configure\n",
    "system_prompt = \"\"\"\n",
    "You are an expert chef recommending delicious and useful recipes. Present only one recipe at a time. If the user doesn't specify what ingredients they have available, assume only basic ingredients are available.Be descriptive in the steps of the recipe, so it is easy to follow.Have variety in your recipes, don't just recommend the same thing over and over.You MUST suggest a complete recipe; don't ask follow-up questions.Mention the serving size in the recipe. If not specified, assume 2 people.\n",
    "\"\"\"\n",
    "\n",
    "# Start by getting the prompt if it already exists.\n",
    "# If it does, we can delete it and re-create, if not we create it.\n",
    "prompt = get_prompt(name=PROMPT_NAME)\n",
    "\n",
    "if prompt is not None:\n",
    "    print(f\"Prompt already exists with ID: {prompt.id}, deleting it to re-create.\")\n",
    "    prompt = delete_prompt(name=PROMPT_NAME)\n",
    "\n",
    "prompt = create_prompt(\n",
    "    name=PROMPT_NAME,\n",
    "    template=[\n",
    "        Message(\n",
    "            role=MessageRole.system,\n",
    "            content=system_prompt,\n",
    "        ),\n",
    "        Message(role=MessageRole.user, content=\"{{input}}\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Output a link to view the prompt in Galileo\n",
    "print(f\"Prompt created. You can view it at {os.environ.get('GALILEO_CONSOLE_URL', 'https://app.galileo.ai/').removesuffix('/')}/prompts/{prompt.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d362e5",
   "metadata": {},
   "source": [
    "Next we can generate the dataset using the queries you generated in the last section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ef0080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from galileo.datasets import get_dataset, create_dataset, delete_dataset\n",
    "\n",
    "# Now we have the CSV file loaded, lets create a dataset. If the dataset already exists, we will delete it and re-create it.\n",
    "dataset = get_dataset(\n",
    "    name=DATASET_NAME\n",
    ")\n",
    "\n",
    "if dataset is not None:\n",
    "    print(f\"Dataset already exists with ID: {dataset.id}, deleting it to re-create.\")\n",
    "    dataset = delete_dataset(\n",
    "        name=DATASET_NAME\n",
    "    )\n",
    "\n",
    "dataset = create_dataset(\n",
    "    name=DATASET_NAME,\n",
    "    content=[{\"input\": q} for q in queries],\n",
    ")\n",
    "\n",
    "print(f\"Dataset created. You can view it at {os.environ.get('GALILEO_CONSOLE_URL', 'https://app.galileo.ai/').removesuffix('/')}/datasets/{dataset.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81365d9c",
   "metadata": {},
   "source": [
    "Now we can use the prompt and dataset to generate the responses by running an experiment. Once the experiment has started, a link to the experiment will be in the output. Follow this link to monitor the progress of the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f748a5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from galileo.experiments import run_experiment\n",
    "from galileo.resources.models import PromptRunSettings\n",
    "\n",
    "# Create the experiment prompt run settings to define the model\n",
    "# Update the model_alias to the model you want to use for the experiment\n",
    "prompt_run_settings = PromptRunSettings(\n",
    "    model_alias=MODEL\n",
    ")\n",
    "\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "experiment_name = f\"Homework 2 Experiment - {current_time}\"\n",
    "\n",
    "# Run the experiment using the prompt and dataset we created\n",
    "results = run_experiment(\n",
    "    experiment_name,\n",
    "    dataset=dataset,\n",
    "    prompt_template=prompt,\n",
    "    project=PROJECT_NAME,\n",
    "    prompt_settings=prompt_run_settings,\n",
    ")\n",
    "\n",
    "print(f\"Experiment has started. You can view the experiment at {results['link']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10ef5b6",
   "metadata": {},
   "source": [
    "### Open coding\n",
    "\n",
    "Once the experiment is complete, it's time to use open coding to add detailed notes to each trace looking for errors or anything unusual. Galileo allows you to define annotations that you can then add to each trace, and in this case use annotations for open coding.\n",
    "\n",
    "#### Define your annotations\n",
    "\n",
    "Annotations are defined up front, so that anyone can consistently annotate a trace. Annotations can be text, categories, scores, or star or thumbs up/down ratings. You can also provide details on the criteria for the annotation, so that a domain expert can detail how an annotator should approach annotating these data, as well as providing a way for the annotator to note details or the rationale for their annotation.\n",
    "\n",
    "To define an annotation:\n",
    "\n",
    "1. Select the **Annotations** section from the Galileo sidebar for the \"AI Evals Course - Homework 2\" project. Then select **Add your first annotation**.\n",
    "\n",
    "    <div>\n",
    "    <img src=\"./images/annotations-sidebar.webp\" width=\"550\"/>\n",
    "    </div>\n",
    "\n",
    "1. Name the annotation \"Open coding\", and set the **Annotation type** to **Text**. Then select **Save to project**.\n",
    "\n",
    "    <div>\n",
    "    <img src=\"./images/create-annotation.webp\" width=\"500\"/>\n",
    "    </div>\n",
    "\n",
    "This annotation is now ready to use in your project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b270700e",
   "metadata": {},
   "source": [
    "#### Review and annotate your experiment\n",
    "\n",
    "When you ran the experiment earlier in this notebook, traces were generated for each query, showing the input and output that was sent to the LLM. You can now view those traces, and using the annotation you just defined to open code the trace.\n",
    "\n",
    "1. Open the experiment using the link that was output earlier, or selecting it in your project in Galileo. You should see 7 traces.\n",
    "\n",
    "    <div>\n",
    "    <img src=\"./images/experiment-traces.webp\" width=\"800\"/>\n",
    "    </div>\n",
    "\n",
    "1. Select a trace to see the details. You will see a tree showing the trace, with a single LLM span that represents the LLM call using the prompt and the relevant row from the dataset you created earlier. With the trace selected you will also see the input and output for that trace, with the user prompt sent to the LLM, and the response.\n",
    "\n",
    "    <div>\n",
    "    <img src=\"./images/experiment-selected-trace.webp\" width=\"800\"/>\n",
    "    </div>\n",
    "\n",
    "    If you want to see the system prompt as well, select the LLM span.\n",
    "\n",
    "    <div>\n",
    "    <img src=\"./images/experiment-selected-trace-llm-span.webp\" width=\"570\"/>\n",
    "    </div>\n",
    "\n",
    "1. With the trace selected, make sure the **Metrics Pane** is visible, and select **Annotations**.\n",
    "\n",
    "    <div>\n",
    "    <img src=\"./images/annotations-metric-pane.webp\" width=\"570\"/>\n",
    "    </div>\n",
    "\n",
    "1. Enter your annotation, noting any errors, inconsistencies, or any relevant information about how the output could be improved. Enter complete sentences with details as these will be sent to an LLM later to build failure modes.\n",
    "\n",
    "    The annotation is automatically saved.\n",
    "1. Continue the process for the rest of the traces. You can quickly navigate between traces using the arrows at the top.\n",
    "\n",
    "    <div>\n",
    "    <img src=\"./images/traces-navigation.webp\" width=\"250\"/>\n",
    "    </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f94d4c",
   "metadata": {},
   "source": [
    "### Build your taxonomy\n",
    "\n",
    "These annotations form the basis of the failure modes you now need to define. You can use these annotation to come up with two or more failure modes.\n",
    "\n",
    "An easy way to get started doing this is to use an LLM to review the open codes and suggest failure modes. You can then use the LLM to define the failure mode of each trace based on the annotations. This way you can leverage an AI to look for patterns and groupings across your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5586213f",
   "metadata": {},
   "source": [
    "#### Export your data\n",
    "\n",
    "The easiest way to quickly visualize and create failure modes is to export the annotations with the input and output data, then you can pass this to an LLM to create failure modes.\n",
    "\n",
    "1. From the experiment, select all the traces using the checkbox at the start of the column.\n",
    "\n",
    "    <div>\n",
    "    <img src=\"./images/selected-traces.webp\" width=\"800\"/>\n",
    "    </div>\n",
    "\n",
    "1. Select the **Export** button.\n",
    "1. From the **Export Data** dialog, make sure the **Input**, **Output**, and **Open Coding** columns are selected. Give the exported file a name. then select the **Export** button to save the experiments as a CSV file.\n",
    "\n",
    "    <div>\n",
    "    <img src=\"./images/export-dialog.webp\" width=\"500\"/>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e319eb7b",
   "metadata": {},
   "source": [
    "#### Build the failure modes using an LLM\n",
    "\n",
    "Once you have exported your data, you can use an LLM to create a set of failure modes, and assign these to each row of the export.\n",
    "\n",
    "Start by loading the file you exported. Update the `EXPORT_FILE` constant below to map to the file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0144ee5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update this to reflect the file you exported your results to\n",
    "EXPORT_FILE = \"./export.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd8ed40",
   "metadata": {},
   "source": [
    "Now load the file, extracting the input, output, and Open Coding columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be26c8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "exported_data = []\n",
    "with open(EXPORT_FILE, mode=\"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    if not reader.fieldnames:\n",
    "        raise ValueError(\"CSV has no header\")\n",
    "\n",
    "    required = [\"input\", \"output\", \"feedback/Open Coding\"]\n",
    "    missing = [c for c in required if c not in reader.fieldnames]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    for row in reader:\n",
    "        item = {\n",
    "            \"input\": (row.get(\"input\") or \"\").strip(),\n",
    "            \"output\": (row.get(\"output\") or \"\").strip(),\n",
    "            \"open_coding\": (row.get(\"feedback/Open Coding\") or \"\").strip(),\n",
    "        }\n",
    "        exported_data.append(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1cc950",
   "metadata": {},
   "source": [
    "Next we can pass this to an LLM and ask it to create the failure modes for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080e0dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt to generate the failure modes\n",
    "prompt = f\"\"\"You are analyzing failures in a recipe bot. This bot has the following system prompt:\n",
    "\n",
    "<system prompt>\n",
    "{system_prompt}\n",
    "</system prompt>\n",
    "\n",
    "Analyze the following interactions with the bot. This data contains the input set to the bot, the bot's output, and human open coding feedback about the failure modes observed.\n",
    "\n",
    "{exported_data}\n",
    "\n",
    "Based on patterns you see in the data, generate a list of the top five most common failure modes observed in the bot's outputs. For each failure mode provide:\n",
    "- A one or two word name\n",
    "- A brief description of the failure mode\n",
    "- A list of all the inputs from the data that exhibit this failure mode. You can just provide the name of the recipes, not the entire input.\n",
    "\n",
    "Return this as a JSON array in this format:\n",
    "\n",
    "[\n",
    "    {{\n",
    "        \"name\": \"Failure Mode Name\",\n",
    "        \"description\": \"Brief description of the failure mode.\",\n",
    "        \"examples\": [\"Chicken pasta\", \"Ramen\"]\n",
    "    }}\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# Get the response from OpenAI\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant analyzes failure modes in AI applications.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Parse the response as JSON\n",
    "failure_modes = json.loads(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ed002b",
   "metadata": {},
   "source": [
    "Finally we can output the failure modes in a more formatted fashion, saving them to a markdown file called `failure_mode_taxonomy.md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1719c3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "md_lines = [\n",
    "    \"# Failure Mode Taxonomy\",\n",
    "    \"\",\n",
    "    \"This document outlines the failure modes observed or anticipated for the Recipe Chatbot. Each failure mode includes a title, a concise definition, and illustrative examples.\",\n",
    "    \"\"\n",
    "]\n",
    "for i, fm in enumerate(failure_modes, start=1):\n",
    "    md_lines.append(f\"## Failure Mode {i}: {fm['name']}\")\n",
    "    md_lines.append(\"\")\n",
    "    md_lines.append(f\"* **Definition**: {fm['description']}\")\n",
    "    md_lines.append(\"* **Illustrative Examples**:\")\n",
    "    for j, ex in enumerate(fm.get(\"examples\", []), start=1):\n",
    "        md_lines.append(f\"  {j}. {ex}\")\n",
    "    md_lines.append(\"\")\n",
    "\n",
    "content = \"\\n\".join(md_lines)\n",
    "Path(\"failure_mode_taxonomy.md\").write_text(content, encoding=\"utf-8\")\n",
    "print(\"Wrote failure_mode_taxonomy.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52dd057",
   "metadata": {},
   "source": [
    "## Track it\n",
    "\n",
    "The final step is to track the new failure modes, assigning them as appropriate to the different rows in the experiment. We can do this by creating a new annotation that has a category type, then annotating each row with the relevant category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc9bc58",
   "metadata": {},
   "source": [
    "### Create the annotation\n",
    "\n",
    "1. Create a new annotation in the project. Call it \"Failure Mode\"\n",
    "1. Set the **Annotation type** to **Categories**\n",
    "1. Add categories for each of the failure modes\n",
    "\n",
    "    <div>\n",
    "    <img src=\"./images/annotation-failure-mode.webp\" width=\"500\"/>\n",
    "    </div>\n",
    "\n",
    "1. Save the annotation to the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8d69b2",
   "metadata": {},
   "source": [
    "### Annotate the experiment\n",
    "\n",
    "Once the annotation has been created, you can work through the experiment, assigning the relevant failure modes to each trace. You can select one or more failure modes as applicable.\n",
    "\n",
    "<div>\n",
    "<img src=\"./images/trace-with-failure-mode.webp\" width=\"1000\"/>\n",
    "</div>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
