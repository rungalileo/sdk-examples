{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4efa4072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from galileo.handlers.langchain import GalileoCallback\n",
    "from galileo import GalileoDecorator\n",
    "\n",
    "os.environ[\"GALILEO_PROJECT\"] = \"demo-simple-agent-langgraph\"\n",
    "os.environ[\"GALILEO_LOG_STREAM\"]=\"dev\"\n",
    "\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=\"galileo-demo-simple-agent-langgraph\"\n",
    "\n",
    "# Create a callback with the custom logger\n",
    "galileo_v2_callback = GalileoCallback(\n",
    "    start_new_trace=True,   # Whether to start a new trace for each chain\n",
    "    flush_on_chain_end=True # Whether to flush traces when chains end\n",
    ") \n",
    "galileo_context = GalileoDecorator()\n",
    "\n",
    "from braintrust import init_logger\n",
    "from braintrust_langchain import BraintrustCallbackHandler\n",
    "\n",
    "init_logger(project=\"galileo-simple-agent-langgraph\", api_key=os.environ.get(\"BRAINTRUST_API_KEY\"))\n",
    "braintrust_callback = BraintrustCallbackHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04f0e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.types import interrupt, Command\n",
    "\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    # Messages have the type \"list\". The `add_messages` function\n",
    "    # in the annotation defines how this state key should be updated\n",
    "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b128bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b7c5f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized client\n",
      "Obtained async tools [StructuredTool(name='add', description='Add two numbers.\\n    \\n    Args:\\n        a: The first number to add\\n        b: The second number to add\\n        \\n    Returns:\\n        The sum of a and b\\n        \\n    Examples:\\n        To add 5 and 3, provide a=5, b=3\\n    ', args_schema={'properties': {'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}, 'required': ['a', 'b'], 'title': 'addArguments', 'type': 'object'}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x158bf9d00>), StructuredTool(name='multiply', description='Multiply two numbers.\\n    \\n    Args:\\n        a: The first number to multiply\\n        b: The second number to multiply\\n        \\n    Returns:\\n        The product of a and b\\n        \\n    Examples:\\n        To multiply 5 and 3, provide a=5, b=3. Do not use expressions like \"5*3\".\\n    ', args_schema={'properties': {'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}, 'required': ['a', 'b'], 'title': 'multiplyArguments', 'type': 'object'}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x158bf9da0>), StructuredTool(name='get_weather', description='Get the current weather for a location.\\n    \\n    Args:\\n        location: The city or location to get weather for (e.g., \"New York\", \"Paris\")\\n        \\n    Returns:\\n        A string describing the current weather conditions\\n        \\n    Examples:\\n        To get weather for London, provide location=\"London\"\\n    ', args_schema={'properties': {'location': {'title': 'Location', 'type': 'string'}}, 'required': ['location'], 'title': 'get_weatherArguments', 'type': 'object'}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x158bfaa20>)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x120ee6b40>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import asyncio\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain_core.tools import BaseTool, StructuredTool, Tool\n",
    "from typing import Any, Dict, Optional, Union, Callable\n",
    "\n",
    "llm = init_chat_model(\"openai:gpt-4.1\", callbacks=[galileo_v2_callback, braintrust_callback])\n",
    "\n",
    "@tool\n",
    "def human_assistance(query: str) -> str:\n",
    "    \"\"\"Request assistance from a human.\"\"\"\n",
    "    human_response = interrupt({\"query\": query})\n",
    "    return human_response[\"data\"]\n",
    "\n",
    "math_server_path = \"/Users/sid/Desktop/galileo/sdk-examples/python/mcp/example_math_server.py\"\n",
    "weather_server_path = \"/Users/sid/Desktop/galileo/sdk-examples/python/mcp/example_weather_server.py\"\n",
    "\n",
    "# def create_async_to_sync_wrapper(async_tool: BaseTool) -> BaseTool:\n",
    "#     \"\"\"Results in:\n",
    "\n",
    "#     What is 23 times 43?\n",
    "#     ================================== Ai Message ==================================\n",
    "#     Tool Calls:\n",
    "#     multiply (call_GKQvF422F0rxYDz5Se7IR0hB)\n",
    "#     Call ID: call_GKQvF422F0rxYDz5Se7IR0hB\n",
    "#     Args:\n",
    "#         __arg1: 23*43\n",
    "#     ================================= Tool Message =================================\n",
    "#     Name: multiply\n",
    "\n",
    "#     Error: TypeError('create_async_to_sync_wrapper.<locals>.sync_func() takes 0 positional arguments but 1 was given')\n",
    "#     Please fix your mistakes.\n",
    "#     ================================== Ai Message ==================================\n",
    "#     Tool Calls:\n",
    "#     multiply (call_qxAPCoNIVi1zwpceYZHqFR2W)\n",
    "#     Call ID: call_qxAPCoNIVi1zwpceYZHqFR2W\n",
    "#     Args:\n",
    "#         __arg1: 23\n",
    "#     ================================= Tool Message =================================\n",
    "#     Name: multiply\n",
    "\n",
    "#     Error: TypeError('create_async_to_sync_wrapper.<locals>.sync_func() takes 0 positional arguments but 1 was given')\n",
    "#     Please fix your mistakes.\n",
    "#     ================================== Ai Message ==================================\n",
    "\n",
    "#     I made an error in formatting the input for multiplication. Let me correct that and calculate 23 times 43 for you.\n",
    "#     Tool Calls:\n",
    "#     multiply (call_SME9y7xAtQ7T2g46p6rQYQ3j)\n",
    "#     Call ID: call_SME9y7xAtQ7T2g46p6rQYQ3j\n",
    "#     Args:\n",
    "#         __arg1: 23,43\n",
    "#     ================================= Tool Message =================================\n",
    "#     Name: multiply\n",
    "\n",
    "#     Error: TypeError('create_async_to_sync_wrapper.<locals>.sync_func() takes 0 positional arguments but 1 was given')\n",
    "#     Please fix your mistakes.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Create sync function that correctly packages arguments for ainvoke\n",
    "#     def sync_func(**kwargs) -> Any:\n",
    "#         loop = asyncio.get_event_loop()\n",
    "        \n",
    "#         # Remove callbacks if present\n",
    "#         input_dict = {k: v for k, v in kwargs.items() if k != 'callbacks'}\n",
    "        \n",
    "#         # Call ainvoke with the properly formatted input parameter\n",
    "#         try:\n",
    "#             if hasattr(async_tool, 'ainvoke') and callable(async_tool.ainvoke):\n",
    "#                 # This is the key fix - ainvoke expects a single 'input' parameter \n",
    "#                 # that contains the dictionary of arguments\n",
    "#                 return loop.run_until_complete(async_tool.ainvoke(input=input_dict))\n",
    "#             else:\n",
    "#                 raise ValueError(f\"Tool {async_tool.name} doesn't have an ainvoke method\")\n",
    "#         except Exception as e:\n",
    "#             return f\"Error executing {async_tool.name}: {str(e)}\"\n",
    "    \n",
    "#     # Always use a basic Tool for wrapper - this avoids schema issues\n",
    "#     return Tool(\n",
    "#         name=async_tool.name,\n",
    "#         description=async_tool.description,\n",
    "#         func=sync_func\n",
    "#     )\n",
    "\n",
    "\n",
    "# def create_async_to_sync_wrapper(async_tool: BaseTool) -> BaseTool:\n",
    "#     \"\"\"Results in:\n",
    "#     What is 23 times 43?\n",
    "#     ================================== Ai Message ==================================\n",
    "#     Tool Calls:\n",
    "#     multiply (call_8RgKaCBTj6Oh4br1dm1qq67s)\n",
    "#     Call ID: call_8RgKaCBTj6Oh4br1dm1qq67s\n",
    "#     Args:\n",
    "#         __arg1: 23 times 43\n",
    "#     ================================= Tool Message =================================\n",
    "#     Name: multiply\n",
    "\n",
    "#     Error: TypeError('create_async_to_sync_wrapper.<locals>.sync_func() takes 0 positional arguments but 1 was given')\n",
    "#     Please fix your mistakes.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def sync_func(**kwargs):\n",
    "#         loop = asyncio.get_event_loop()\n",
    "#         try:\n",
    "#             return loop.run_until_complete(async_tool.ainvoke(input=kwargs))\n",
    "#         except Exception as e:\n",
    "#             return f\"Error executing {async_tool.name}: {str(e)}\"\n",
    "    \n",
    "#     return Tool(\n",
    "#         name=async_tool.name,\n",
    "#         description=async_tool.description,\n",
    "#         func=sync_func\n",
    "#     )\n",
    "\n",
    "# def create_async_to_sync_wrapper(async_tool: BaseTool) -> BaseTool:\n",
    "#     \"\"\"Results in:\n",
    "\n",
    "#     What is 23 times 43?\n",
    "#     ================================== Ai Message ==================================\n",
    "#     Tool Calls:\n",
    "#     multiply (call_ZKmc4zZcc9AsoBExfRAP8zk0)\n",
    "#     Call ID: call_ZKmc4zZcc9AsoBExfRAP8zk0\n",
    "#     Args:\n",
    "#         __arg1: 23 43\n",
    "#     Tool multiply called with args: ('23 43',), kwargs: {}\n",
    "#     ================================= Tool Message =================================\n",
    "#     Name: multiply\n",
    "\n",
    "#     Error: Tool multiply execution timed out\n",
    "#     ================================== Ai Message ==================================\n",
    "#     Tool Calls:\n",
    "#     multiply (call_I9EpBlzkEQnDflJoueGi7dmr)\n",
    "#     Call ID: call_I9EpBlzkEQnDflJoueGi7dmr\n",
    "#     Args:\n",
    "#         __arg1: 23\n",
    "#     Tool multiply called with args: ('23',), kwargs: {}\n",
    "#     ================================= Tool Message =================================\n",
    "#     Name: multiply\n",
    "\n",
    "#     Error: Tool multiply execution timed out\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def sync_func(*args, **kwargs):\n",
    "#         # Print args and kwargs for debugging\n",
    "#         print(f\"Tool {async_tool.name} called with args: {args}, kwargs: {kwargs}\")\n",
    "        \n",
    "#         # For LangGraph compatibility, accept both positional and keyword arguments\n",
    "#         # But convert to a proper input dictionary for ainvoke\n",
    "#         if args and len(args) == 1 and isinstance(args[0], dict):\n",
    "#             # If first argument is a dict, use it as the input\n",
    "#             input_dict = args[0]\n",
    "#         else:\n",
    "#             # Otherwise use kwargs as the input\n",
    "#             input_dict = kwargs\n",
    "        \n",
    "#         # Remove callbacks to avoid serialization issues\n",
    "#         if 'callbacks' in input_dict:\n",
    "#             input_dict = {k: v for k, v in input_dict.items() if k != 'callbacks'}\n",
    "            \n",
    "#         # Get the event loop\n",
    "#         loop = asyncio.get_event_loop()\n",
    "        \n",
    "#         try:\n",
    "#             # Call the async tool with the input dictionary\n",
    "#             coro = async_tool.ainvoke(input=input_dict)\n",
    "#             result = loop.run_until_complete(\n",
    "#                 asyncio.wait_for(coro, timeout=5.0)\n",
    "#             )\n",
    "#             return result\n",
    "#         except asyncio.TimeoutError:\n",
    "#             return f\"Error: Tool {async_tool.name} execution timed out\"\n",
    "#         except Exception as e:\n",
    "#             # Use a simplified error message for clarity\n",
    "#             error_msg = f\"Error executing {async_tool.name}: {type(e).__name__} - {str(e)}\"\n",
    "#             print(error_msg)\n",
    "#             return error_msg\n",
    "    \n",
    "#     # Create a basic Tool with the sync function\n",
    "#     return Tool(\n",
    "#         name=async_tool.name,\n",
    "#         description=async_tool.description,\n",
    "#         func=sync_func\n",
    "#     )\n",
    "\n",
    "\n",
    "# def create_async_to_sync_wrapper(async_tool: BaseTool) -> BaseTool:\n",
    "#     \"\"\"Results in:\n",
    "\n",
    "#     What is 23 times 43?\n",
    "#     ================================== Ai Message ==================================\n",
    "#     Tool Calls:\n",
    "#     multiply (call_u19OA3uNqsyFrMJ15oNyP1GS)\n",
    "#     Call ID: call_u19OA3uNqsyFrMJ15oNyP1GS\n",
    "#     Args:\n",
    "#         __arg1: 23 43\n",
    "#     ================================= Tool Message =================================\n",
    "#     Name: multiply\n",
    "\n",
    "#     Error: AttributeError(\"'dict' object has no attribute 'schema'\")\n",
    "#     Please fix your mistakes.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def sync_func(*args, **kwargs):\n",
    "#         # Process inputs - handle both dict as first arg (from ToolNode) and kwargs\n",
    "#         input_dict = {}\n",
    "        \n",
    "#         # If we got a dictionary as first positional arg (LangGraph style)\n",
    "#         if args and len(args) == 1 and isinstance(args[0], dict):\n",
    "#             input_dict.update(args[0])\n",
    "#         # Otherwise use keyword arguments\n",
    "#         else:\n",
    "#             input_dict.update(kwargs)\n",
    "            \n",
    "#         # Remove callbacks to avoid serialization issues\n",
    "#         if 'callbacks' in input_dict:\n",
    "#             input_dict.pop('callbacks')\n",
    "            \n",
    "#         # Get schema properties to extract param names if available\n",
    "#         arg_names = []\n",
    "#         if hasattr(async_tool, 'args_schema'):\n",
    "#             schema = async_tool.args_schema.schema()\n",
    "#             if 'properties' in schema:\n",
    "#                 arg_names = list(schema['properties'].keys())\n",
    "                \n",
    "#         # Extract values from __arg parameters and map to schema params\n",
    "#         for key in list(input_dict.keys()):\n",
    "#             if key.startswith('__arg') and arg_names:\n",
    "#                 value = input_dict.pop(key)\n",
    "#                 # Try to find the correct parameter by index\n",
    "#                 index = 0\n",
    "#                 try:\n",
    "#                     # Extract number from __arg1, __arg2, etc.\n",
    "#                     arg_num = int(key.replace('__arg', ''))\n",
    "#                     index = arg_num - 1  # Convert to 0-based index\n",
    "#                 except (ValueError, IndexError):\n",
    "#                     pass\n",
    "                \n",
    "#                 # If we have a valid index and there's a matching parameter name\n",
    "#                 if 0 <= index < len(arg_names):\n",
    "#                     input_dict[arg_names[index]] = value\n",
    "        \n",
    "#         # Get event loop\n",
    "#         loop = asyncio.get_event_loop()\n",
    "        \n",
    "#         try:\n",
    "#             # Call the async tool with the input dictionary\n",
    "#             if hasattr(async_tool, 'ainvoke') and callable(async_tool.ainvoke):\n",
    "#                 return loop.run_until_complete(\n",
    "#                     asyncio.wait_for(async_tool.ainvoke(input=input_dict), timeout=5.0)\n",
    "#                 )\n",
    "#             else:\n",
    "#                 return f\"Error: {async_tool.name} doesn't have an ainvoke method\"\n",
    "#         except asyncio.TimeoutError:\n",
    "#             return f\"Error executing {async_tool.name}: Operation timed out\"\n",
    "#         except Exception as e:\n",
    "#             return f\"Error executing {async_tool.name}: {str(e)}\"\n",
    "    \n",
    "#     # Create a basic Tool with the sync function\n",
    "#     return Tool(\n",
    "#         name=async_tool.name,\n",
    "#         description=async_tool.description,\n",
    "#         func=sync_func\n",
    "#     )\n",
    "\n",
    "# def create_async_to_sync_wrapper(async_tool: BaseTool) -> BaseTool:\n",
    "#     \"\"\"Results in:\n",
    "\n",
    "#     What is the weather in New York?\n",
    "#     ================================== Ai Message ==================================\n",
    "#     Tool Calls:\n",
    "#     get_weather (call_0FMbHBblnQlvlpDdVR1PGYcO)\n",
    "#     Call ID: call_0FMbHBblnQlvlpDdVR1PGYcO\n",
    "#     Args:\n",
    "#         location: New York\n",
    "#     ================================= Tool Message =================================\n",
    "#     Name: get_weather\n",
    "\n",
    "#     Error executing get_weather: Operation timed out\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def sync_func(*args, **kwargs):\n",
    "#         # Handle both direct calls and calls via LangGraph's ToolNode\n",
    "#         input_dict = {}\n",
    "        \n",
    "#         # If called with a dictionary as first positional argument (LangGraph pattern)\n",
    "#         if args and len(args) == 1 and isinstance(args[0], dict):\n",
    "#             input_dict = args[0]\n",
    "#         else:\n",
    "#             input_dict = kwargs\n",
    "        \n",
    "#         # Remove callbacks if present\n",
    "#         if 'callbacks' in input_dict:\n",
    "#             input_dict.pop('callbacks')\n",
    "        \n",
    "#         # Get event loop\n",
    "#         loop = asyncio.get_event_loop()\n",
    "        \n",
    "#         try:\n",
    "#             # Pass the input dictionary to the tool's ainvoke method\n",
    "#             if hasattr(async_tool, 'ainvoke') and callable(async_tool.ainvoke):\n",
    "#                 return loop.run_until_complete(\n",
    "#                     asyncio.wait_for(async_tool.ainvoke(input=input_dict), timeout=5.0)\n",
    "#                 )\n",
    "#             else:\n",
    "#                 return f\"Error: {async_tool.name} doesn't have an ainvoke method\"\n",
    "#         except asyncio.TimeoutError:\n",
    "#             return f\"Error executing {async_tool.name}: Operation timed out\"\n",
    "#         except Exception as e:\n",
    "#             return f\"Error executing {async_tool.name}: {str(e)}\"\n",
    "    \n",
    "#     # Here's the key improvement - create a structured tool that preserves the schema\n",
    "#     # This ensures the LLM gets proper parameter information\n",
    "#     return StructuredTool.from_function(\n",
    "#         func=sync_func,\n",
    "#         name=async_tool.name,\n",
    "#         description=async_tool.description,\n",
    "#         args_schema=async_tool.args_schema if hasattr(async_tool, \"args_schema\") else None\n",
    "#     )\n",
    "\n",
    "# def create_async_to_sync_wrapper(async_tool: BaseTool) -> BaseTool:\n",
    "#     \"\"\"Create a synchronous wrapper for an async tool that handles LangGraph's calling convention.\"\"\"\n",
    "    \n",
    "#     def sync_func(*args, **kwargs):\n",
    "#         # Handle both direct calls and calls via LangGraph's ToolNode\n",
    "#         input_dict = {}\n",
    "        \n",
    "#         # If called with a dictionary as first positional argument (LangGraph pattern)\n",
    "#         if args and len(args) == 1 and isinstance(args[0], dict):\n",
    "#             input_dict = args[0]\n",
    "#         else:\n",
    "#             input_dict = kwargs\n",
    "        \n",
    "#         # Remove callbacks if present\n",
    "#         if 'callbacks' in input_dict:\n",
    "#             input_dict.pop('callbacks')\n",
    "        \n",
    "#         # Print debug info\n",
    "#         print(f\"Tool {async_tool.name} called with input: {input_dict}\")\n",
    "        \n",
    "#         # Instead of trying to call the async version which is timing out,\n",
    "#         # implement the math functions directly\n",
    "#         if async_tool.name == \"multiply\" and \"a\" in input_dict and \"b\" in input_dict:\n",
    "#             try:\n",
    "#                 return int(input_dict[\"a\"]) * int(input_dict[\"b\"])\n",
    "#             except (ValueError, TypeError):\n",
    "#                 return f\"Error: parameters must be numbers, got a={input_dict['a']}, b={input_dict['b']}\"\n",
    "        \n",
    "#         elif async_tool.name == \"add\" and \"a\" in input_dict and \"b\" in input_dict:\n",
    "#             try:\n",
    "#                 return int(input_dict[\"a\"]) + int(input_dict[\"b\"])\n",
    "#             except (ValueError, TypeError):\n",
    "#                 return f\"Error: parameters must be numbers, got a={input_dict['a']}, b={input_dict['b']}\"\n",
    "        \n",
    "#         elif async_tool.name == \"get_weather\" and \"location\" in input_dict:\n",
    "#             return f\"It's sunny in {input_dict['location']}\"\n",
    "        \n",
    "#         else:\n",
    "#             return f\"Unsupported operation or missing parameters for {async_tool.name}\"\n",
    "    \n",
    "#     # Create a tool that preserves the name and description\n",
    "#     return Tool(\n",
    "#         name=async_tool.name,\n",
    "#         description=async_tool.description,\n",
    "#         func=sync_func\n",
    "#     )\n",
    "\n",
    "# def create_async_to_sync_wrapper(async_tool: BaseTool) -> BaseTool:\n",
    "#     \"\"\"Create a synchronous wrapper for an async tool.\"\"\"\n",
    "    \n",
    "#     def sync_func(*args, **kwargs):\n",
    "#         # Handle both positional dictionary (from LangGraph) and keyword args\n",
    "#         input_dict = {}\n",
    "        \n",
    "#         # If called with a dict as first arg (LangGraph pattern)\n",
    "#         if args and len(args) == 1 and isinstance(args[0], dict):\n",
    "#             input_dict = args[0]\n",
    "#         else:\n",
    "#             input_dict = kwargs\n",
    "        \n",
    "#         # Map __arg1 to the first parameter if needed\n",
    "#         if '__arg1' in input_dict and hasattr(async_tool, 'args_schema'):\n",
    "#             schema = async_tool.args_schema.schema()\n",
    "#             if 'properties' in schema and schema['properties']:\n",
    "#                 # Get the first parameter name\n",
    "#                 first_param = next(iter(schema['properties'].keys()))\n",
    "#                 input_dict[first_param] = input_dict.pop('__arg1')\n",
    "        \n",
    "#         # Get event loop\n",
    "#         loop = asyncio.get_event_loop()\n",
    "        \n",
    "#         try:\n",
    "#             # Use .invoke instead of .ainvoke for debugging\n",
    "#             # This directly uses the same interface that would be called when using the tool normally\n",
    "#             return loop.run_until_complete(\n",
    "#                 asyncio.wait_for(async_tool.ainvoke(input=input_dict), timeout=10.0)\n",
    "#             )\n",
    "#         except asyncio.TimeoutError:\n",
    "#             return f\"Error: Tool {async_tool.name} timed out after 10 seconds\"\n",
    "#         except Exception as e:\n",
    "#             return f\"Error executing {async_tool.name}: {str(e)}\"\n",
    "    \n",
    "#     # Create a basic Tool\n",
    "#     return Tool(\n",
    "#         name=async_tool.name,\n",
    "#         description=async_tool.description,\n",
    "#         func=sync_func\n",
    "#     )\n",
    "\n",
    "def create_async_to_sync_wrapper(async_tool: BaseTool) -> BaseTool:\n",
    "    \"\"\"Create a synchronous wrapper for an async tool that avoids blocking.\"\"\"\n",
    "    \n",
    "    def sync_func(*args, **kwargs):\n",
    "        # Handle both positional dictionary (from LangGraph) and keyword args\n",
    "        input_dict = {}\n",
    "        \n",
    "        # If called with a dict as first arg (LangGraph pattern)\n",
    "        if args and len(args) == 1 and isinstance(args[0], dict):\n",
    "            input_dict = args[0].copy()\n",
    "        else:\n",
    "            input_dict = kwargs.copy()\n",
    "        \n",
    "        # Map __arg1 to the first parameter if needed\n",
    "        if '__arg1' in input_dict and hasattr(async_tool, 'args_schema'):\n",
    "            schema = async_tool.args_schema.schema()\n",
    "            if 'properties' in schema and schema['properties']:\n",
    "                # Get the first parameter name\n",
    "                first_param = next(iter(schema['properties'].keys()))\n",
    "                input_dict[first_param] = input_dict.pop('__arg1')\n",
    "        \n",
    "        print(f\"Executing {async_tool.name} with input: {input_dict}\")\n",
    "        \n",
    "        # Create a dedicated event loop for this tool call\n",
    "        tool_loop = asyncio.new_event_loop()\n",
    "        \n",
    "        try:\n",
    "            # Run the tool in the dedicated loop\n",
    "            result = tool_loop.run_until_complete(\n",
    "                asyncio.wait_for(\n",
    "                    async_tool.ainvoke(input=input_dict),\n",
    "                    timeout=5.0\n",
    "                )\n",
    "            )\n",
    "            print(f\"Tool {async_tool.name} result: {result}\")\n",
    "            return result\n",
    "        except asyncio.TimeoutError:\n",
    "            print(f\"Tool {async_tool.name} timed out\")\n",
    "            return f\"Error: Tool {async_tool.name} timed out after 5 seconds\"\n",
    "        except Exception as e:\n",
    "            print(f\"Tool {async_tool.name} error: {type(e).__name__}: {e}\")\n",
    "            return f\"Error executing {async_tool.name}: {str(e)}\"\n",
    "        finally:\n",
    "            # Properly close the event loop to free resources\n",
    "            tool_loop.close()\n",
    "    \n",
    "    return Tool(\n",
    "        name=async_tool.name,\n",
    "        description=async_tool.description,\n",
    "        func=sync_func\n",
    "    )\n",
    "\n",
    "async def setup_mcp():\n",
    "    client = MultiServerMCPClient(\n",
    "        {\n",
    "            \"math\": {\n",
    "                \"command\": \"python\",\n",
    "                \"args\": [math_server_path],\n",
    "                \"transport\": \"stdio\",\n",
    "            },\n",
    "            \"weather\": {\n",
    "                \"url\": \"http://localhost:8000/sse\",\n",
    "                \"transport\": \"sse\",\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    print(\"Initialized client\")\n",
    "    await client.__aenter__()\n",
    "    \n",
    "    # Get the async MCP tools\n",
    "    async_mcp_tools = client.get_tools()\n",
    "    print(f\"Obtained async tools {async_mcp_tools}\")\n",
    "    \n",
    "    # Convert each async tool to a sync wrapper using our fixed function\n",
    "    sync_mcp_tools = [create_async_to_sync_wrapper(tool) for tool in async_mcp_tools]\n",
    "    \n",
    "    return client, sync_mcp_tools\n",
    "\n",
    "# Run the async setup function\n",
    "loop = asyncio.get_event_loop()\n",
    "mcp_client, mcp_tools = loop.run_until_complete(setup_mcp())\n",
    "\n",
    "# Combine all tools\n",
    "tavily_tool = TavilySearch(max_results=2)\n",
    "tools = [tavily_tool, human_assistance] + mcp_tools\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "def chatbot(state: State):\n",
    "    message = llm_with_tools.invoke(state[\"messages\"])\n",
    "    # Disable parallel tool calling to avoid repeating tool invocations when we resume\n",
    "    assert len(message.tool_calls) <= 1\n",
    "    return {\"messages\": [message]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3b30725",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_node = ToolNode(tools=tools)\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "# The `tools_condition` function returns \"tools\" if the chatbot asks to use a tool, and \"END\" if\n",
    "# it is fine directly responding. This conditional routing defines the main agent loop.\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition\n",
    ")\n",
    "# Any time a tool is called, we return to the chatbot to decide the next step\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71ef5ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    print(\"Something went wrong while trying to display the graph.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb94591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "def stream_with_interrupt_handling():\n",
    "    while True:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Start streaming the response\n",
    "        events = graph.stream(\n",
    "            {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "            config,\n",
    "            stream_mode=\"values\",\n",
    "        )\n",
    "        \n",
    "        # Process events until we hit an interrupt or complete\n",
    "        for event in events:\n",
    "            if \"messages\" in event:\n",
    "                event[\"messages\"][-1].pretty_print()\n",
    "        \n",
    "        # After streaming completes, check if we hit an interrupt\n",
    "        snapshot = graph.get_state(config)\n",
    "        \n",
    "        # Check if the graph is at an interrupt\n",
    "        if snapshot.next:\n",
    "            assert snapshot.next[0] == \"tools\", f\"Unexpected value for snapshot.next: {snapshot.next}\"\n",
    "            # We're at an interrupt - get human input\n",
    "            print(\"\\n[System: Agent is requesting human assistance]\")\n",
    "            \n",
    "            # Extract the query from the interrupt state\n",
    "            interrupt_msg = None\n",
    "            for msg in reversed(snapshot.values[\"messages\"]):\n",
    "                if hasattr(msg, \"tool_calls\") and msg.tool_calls:\n",
    "                    for tool_call in msg.tool_calls:\n",
    "                        if tool_call[\"name\"] == \"human_assistance\":\n",
    "                            interrupt_msg = tool_call[\"args\"][\"query\"]\n",
    "                            break\n",
    "                    if interrupt_msg:\n",
    "                        break\n",
    "            \n",
    "            if interrupt_msg:\n",
    "                print(f\"[Agent query: {interrupt_msg}]\")\n",
    "            \n",
    "            # Get human response\n",
    "            human_response = input(\"Human: \")\n",
    "            \n",
    "            # Resume with the human response\n",
    "            human_command = Command(resume={\"data\": human_response})\n",
    "            \n",
    "            # Continue streaming from where we left off\n",
    "            events = graph.stream(human_command, config, stream_mode=\"values\")\n",
    "            for event in events:\n",
    "                if \"messages\" in event:\n",
    "                    event[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6936f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the weather in London?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_weather (call_VkP2PjxiHXtX7dnO9tBue9vb)\n",
      " Call ID: call_VkP2PjxiHXtX7dnO9tBue9vb\n",
      "  Args:\n",
      "    __arg1: London\n",
      "Executing get_weather with input: {}\n",
      "Tool get_weather timed out\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_weather\n",
      "\n",
      "Error: Tool get_weather timed out after 5 seconds\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I apologize, but I'm currently unable to retrieve the latest weather information for London due to a technical issue. Would you like me to try again or provide some general information about the typical weather in London?\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# snapshot = graph.get_state(config)\n",
    "# print(snapshot)\n",
    "\n",
    "stream_with_interrupt_handling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374bc602",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Notebooks-HO4Jl9IL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
