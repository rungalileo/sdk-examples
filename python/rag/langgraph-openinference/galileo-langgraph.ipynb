{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain & LangGraph Agent with Tool Calling and OTEL Tracing\n",
    "\n",
    "This notebook demonstrates how to build a conversational agent using **LangChain** and **LangGraph**. The agent is equipped with several tools, including a custom retrieval tool built from web pages.\n",
    "\n",
    "Key features highlighted:\n",
    "1.  **Tool Calling**: The agent can use multiple tools, such as a calculator, a weather function, and a document retriever.\n",
    "2.  **RAG (Retrieval-Augmented Generation)**: We build a retriever tool by loading, splitting, and embedding content from Lilian Weng's blog posts.\n",
    "3.  **OpenTelemetry (OTEL) Tracing**: The entire process is instrumented using OpenInference, which automatically captures traces of the LangChain operations. These traces can be exported to a backend like Galileo for monitoring and debugging, or simply printed to the console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "First, we install the necessary libraries. This includes packages for OpenTelemetry, OpenInference, and the various LangChain components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install --upgrade \\\n",
    "  opentelemetry-api opentelemetry-sdk opentelemetry-exporter-otlp opentelemetry-instrumentation \\\n",
    "  openinference-instrumentation-langchain \\\n",
    "  langchain langchain-community langchain-openai langgraph langchain-text-splitters \\\n",
    "  beautifulsoup4 tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Configuration\n",
    "\n",
    "Next, we'll set up the environment variables. You'll need an OpenAI API key. \n",
    "\n",
    "**Tracing with Galileo (Optional)**\n",
    "If you want to export traces to Galileo, you can also provide your Galileo API key, project name, and logstream. If these are not provided, tracing will still be enabled, but traces will only be printed to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Set your OpenAI API key\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OPENAI_API_KEY: \")\n",
    "\n",
    "# Optional: Galileo OTEL export settings\n",
    "# Provide these via env vars to enable tracing export\n",
    "# os.environ[\"GALILEO_API_KEY\"] = getpass(\"Enter GALILEO_API_KEY (or leave blank to skip): \")\n",
    "# os.environ[\"GALILEO_PROJECT\"] = \"your_project\"\n",
    "# os.environ[\"GALILEO_LOGSTREAM\"] = \"default\"\n",
    "# os.environ[\"GALILEO_OTEL_ENDPOINT\"] = \"https://app.galileo.ai/api/galileo/otel/traces\"\n",
    "\n",
    "galileo_api_key = os.getenv(\"GALILEO_API_KEY\", \"\")\n",
    "galileo_project = os.getenv(\"GALILEO_PROJECT\", \"\")\n",
    "galileo_logstream = os.getenv(\"GALILEO_LOGSTREAM\", \"\")\n",
    "otel_endpoint = os.getenv(\"GALILEO_OTEL_ENDPOINT\", \"https://app.galileo.ai/api/galileo/otel/traces\")\n",
    "\n",
    "if galileo_api_key and galileo_project and galileo_logstream:\n",
    "    headers = {\n",
    "        \"Galileo-API-Key\": galileo_api_key,\n",
    "        \"project\": galileo_project,\n",
    "        \"logstream\": galileo_logstream,\n",
    "    }\n",
    "    os.environ[\"OTEL_EXPORTER_OTLP_TRACES_HEADERS\"] = \",\".join([f\"{k}={v}\" for k, v in headers.items()])\n",
    "\n",
    "print(\"Environment configured. Galileo tracing export is\", \"enabled\" if os.getenv(\"OTEL_EXPORTER_OTLP_TRACES_HEADERS\") else \"disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize OpenTelemetry Tracing\n",
    "\n",
    "Here, we set up the OpenTelemetry SDK. We configure a `TracerProvider` which will manage the creation of traces. \n",
    "\n",
    "We add two span processors:\n",
    "1.  **`OTLPSpanExporter`**: This sends traces to an OTLP-compatible endpoint (like Galileo). It's only added if the necessary environment variables are set.\n",
    "2.  **`ConsoleSpanExporter`**: This prints the traces directly to the console, which is useful for local debugging.\n",
    "\n",
    "Finally, `LangChainInstrumentor().instrument()` automatically patches LangChain to create spans for all its operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentelemetry.sdk import trace as trace_sdk\n",
    "from opentelemetry import trace as trace_api\n",
    "from opentelemetry.sdk.resources import Resource\n",
    "from opentelemetry.sdk.trace.export import BatchSpanProcessor, ConsoleSpanExporter\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "from openinference.instrumentation.langchain import LangChainInstrumentor\n",
    "\n",
    "resource = Resource.create({\"service.name\": \"langchain-galileo-notebook\"})\n",
    "tracer_provider = trace_sdk.TracerProvider(resource=resource)\n",
    "\n",
    "# Export to Galileo if configured\n",
    "if os.getenv(\"OTEL_EXPORTER_OTLP_TRACES_HEADERS\"):\n",
    "    tracer_provider.add_span_processor(BatchSpanProcessor(OTLPSpanExporter(endpoint=otel_endpoint)))\n",
    "\n",
    "# Also log spans to console for debugging\n",
    "tracer_provider.add_span_processor(BatchSpanProcessor(ConsoleSpanExporter()))\n",
    "\n",
    "# Register the provider globally\n",
    "trace_api.set_tracer_provider(tracer_provider)\n",
    "\n",
    "# Instrument LangChain\n",
    "LangChainInstrumentor().instrument(tracer_provider=tracer_provider)\n",
    "\n",
    "print(\"LangChain OTEL tracing initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build a Retrieval Tool\n",
    "\n",
    "To give our agent the ability to answer questions about specific documents, we'll create a retrieval tool. This process involves a few standard RAG steps:\n",
    "\n",
    "1.  **Load**: We use `WebBaseLoader` to fetch content from a few blog posts by Lilian Weng.\n",
    "2.  **Split**: We use `RecursiveCharacterTextSplitter` to break the documents into smaller, manageable chunks.\n",
    "3.  **Embed & Store**: We use `OpenAIEmbeddings` to convert the text chunks into vectors and store them in a simple `InMemoryVectorStore`.\n",
    "4.  **Create Tool**: We expose the vector store as a retriever and then wrap it with `create_retriever_tool`, which makes it available for the agent to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "EMBED_MODEL = os.getenv(\"EMBED_MODEL\", \"text-embedding-3-small\")  # or \"text-embedding-3-large\"\n",
    "embeddings = OpenAIEmbeddings(model=EMBED_MODEL)\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-04-12-diffusion-video/\",\n",
    "]\n",
    "\n",
    "# Load web pages\n",
    "docs = []\n",
    "for url in urls:\n",
    "    try:\n",
    "        docs.extend(WebBaseLoader(url).load())\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {url}: {e}\")\n",
    "\n",
    "# Split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=500, chunk_overlap=50)\n",
    "doc_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create in-memory vector store and retriever\n",
    "vectorstore = InMemoryVectorStore.from_documents(doc_splits, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    name=\"retrieve_blog_posts\",\n",
    "    description=\"Search and return information about Lilian Weng blog posts on topics like reward hacking, hallucination, and diffusion models.\",\n",
    ")\n",
    "\n",
    "print(f\"Created a retriever tool with {len(doc_splits)} document splits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create a ReAct Agent\n",
    "\n",
    "Now we define the agent. We'll add two more simple tools (`multiply` and `get_weather`) using the `@tool` decorator to demonstrate how easily custom functions can be integrated.\n",
    "\n",
    "We then use LangGraph's prebuilt `create_react_agent`. This function assembles a graph that follows the ReAct (Reasoning and Acting) logic, allowing the agent to think, use tools, observe the results, and repeat until it reaches a final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers a and b.\"\"\"\n",
    "    print(f\"Multiplying {a} and {b}\")\n",
    "    return a * b\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get the current weather for a given city.\"\"\"\n",
    "    print(f\"Getting weather for {city}\")\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "tools = [get_weather, multiply, retriever_tool]\n",
    "\n",
    "agent_executor = create_react_agent(\n",
    "    model=\"openai:gpt-4o-mini\",\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "print(\"ReAct agent created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run the Agent\n",
    "\n",
    "Finally, let's test the agent with a few different queries. Each query is designed to trigger a different tool, demonstrating the agent's ability to reason and select the correct function for the job.\n",
    "\n",
    "As these run, you will see the full OTEL trace data printed to the console (from our `ConsoleSpanExporter`). If you configured the Galileo exporter, the traces will also be sent there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent_message(message):\n",
    "    result = agent_executor.invoke({\"messages\": [(\"user\", message)]})\n",
    "    final_message = result['messages'][-1]\n",
    "    print(f\"\\nðŸ¤– Final Answer: {final_message.content}\")\n",
    "\n",
    "print(\"--- Query 1: Testing the weather tool ---\")\n",
    "run_agent_message(\"what is the weather in sf\")\n",
    "\n",
    "print(\"\\n--- Query 2: Testing the multiply tool ---\")\n",
    "run_agent_message(\"what is 12 * 666? use the tool\")\n",
    "\n",
    "print(\"\\n--- Query 3: Testing the retriever tool ---\")\n",
    "run_agent_message(\"what does lilian weng's blog say about reward hacking?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}