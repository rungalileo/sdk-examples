{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70ea03f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fecfb7f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "794761fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from galileo.handlers.langchain import GalileoCallback\n",
    "from galileo import GalileoDecorator\n",
    "\n",
    "os.environ[\"GALILEO_PROJECT\"] = \"sessions-demo\"\n",
    "os.environ[\"GALILEO_LOG_STREAM\"]=\"dev\"\n",
    "\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=\"galileo-sessions-demo\"\n",
    "\n",
    "# Create a callback with the custom logger\n",
    "galileo_v2_callback = GalileoCallback(\n",
    "    start_new_trace=True,   # Whether to start a new trace for each chain\n",
    "    flush_on_chain_end=True # Whether to flush traces when chains end\n",
    ") \n",
    "galileo_context = GalileoDecorator()\n",
    "\n",
    "from braintrust import init_logger\n",
    "from braintrust_langchain import BraintrustCallbackHandler\n",
    "\n",
    "init_logger(project=\"Sessions Demo\", api_key=os.environ.get(\"BRAINTRUST_API_KEY\"))\n",
    "braintrust_callback = BraintrustCallbackHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1868cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chat_session():\n",
    "    # Initialize the language model\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", callbacks=[galileo_v2_callback, braintrust_callback])\n",
    "    \n",
    "    # Create a prompt template\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful assistant who responds in a friendly and concise manner.\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "\n",
    "    # Create the chain\n",
    "    chain = (\n",
    "        {\"input\": lambda x: x[\"input\"], \"history\": lambda x: x[\"history\"]}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    galileo_context.start_session(name=\"State capitals\")\n",
    "    return chain\n",
    "\n",
    "def terminate_chat_session():\n",
    "    print(\"\\nAssistant: Goodbye! Have a great day!\")\n",
    "    # galileo_context.flush()\n",
    "\n",
    "def main():\n",
    "    print(\"Welcome to Simple LangChain Chat!\")\n",
    "    print(\"Type 'exit' to end the conversation.\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Create the chat chain\n",
    "    chat_chain = create_chat_session()\n",
    "    \n",
    "    # Initialize conversation history\n",
    "    conversation_history = []\n",
    "    \n",
    "    while True:\n",
    "        # Get user input\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        \n",
    "        # Check if user wants to exit\n",
    "        if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "            terminate_chat_session()\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            # Process the user input through the chain\n",
    "            response = chat_chain.invoke({\n",
    "                \"input\": user_input,\n",
    "                \"history\": conversation_history\n",
    "            })\n",
    "\n",
    "            # Update conversation history\n",
    "            conversation_history.append(HumanMessage(content=user_input))\n",
    "            conversation_history.append(AIMessage(content=response))\n",
    "            \n",
    "            # Display the response\n",
    "            print(f\"\\nAssistant: {response}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3860ad0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Simple LangChain Chat!\n",
      "Type 'exit' to end the conversation.\n",
      "--------------------------------------------------\n",
      "\n",
      "Assistant: The capital of California is Sacramento.\n",
      "\n",
      "Assistant: San Francisco is not the capital of California because Sacramento was chosen as the state's capital in 1854 due to its central location and its proximity to the mining regions of the Sierra Nevada.\n",
      "\n",
      "Assistant: The capital of New York is Albany.\n",
      "\n",
      "Assistant: Albany was chosen as the capital of New York over New York City in 1797 due to its central location within the state and to prevent any one city from having too much power over the government.\n",
      "\n",
      "Assistant: Goodbye! Have a great day!\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df50d65f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Notebooks-HO4Jl9IL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
